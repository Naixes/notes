# 概念
从⼴义上讲，数据结构就是指⼀组数据的存储结构。算法就是操作数据的⼀组⽅法。
从狭义上讲，是指某些著名的数据结构和算法，如队列、栈、堆、二分查找、动态规划等。
# 重点
-  复杂度分析
-  20个基础的数据结构与算法：
10个数据结构：数组、链表、栈、队列、散列表、⼆叉树、堆、跳表、图、Trie树；
10个算法：递归、排序、⼆分查找、搜索、哈希算法、贪⼼算法、分治算法、回溯算法、动态规划、字符串匹配算法。
# 复杂度分析
通过运行代码再进行监控和统计来评价算法的执行时间和内存占用的方法叫事后统计法，受测试环境和数据规模的影响较大，测试结果可能无法反映算法的性能
## 大O表示法
在不运行代码的情况下粗略的估计算法的执行效率
思路：将一行代码的执行时间假设为1个unit_time，计算算法的总执行时间。可以得出结论：代码的总执行时间 T(n) 和每行代码的执行次数 n 成正比，写成公式就是
`T(n) = O(f(n)) // f(n) `表示每行代码执行次数的总和

## 大O时间复杂度
实际上并不具体表示代码真正的执⾏时间，⽽是表示代码执⾏时间随数据规模增⻓的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。
### 分析方法
1. 只关注执行次数最多的一段代码
2. 加法法则：总复杂度等于量级最大的那段代码的复杂度

# 数据结构

## 数组

### 定义

**线性表**数据结构.用一组**连续的内存空间**存储一组**相同类型**的数据

- 线性表:数据排列成线一样的结构,每个数据最多有前后两个方向.比如:栈,链表,队列等

- 非线性表:数据之间不是简单的前后关系

- 连续的内存空间和型同类型的数据:有**随机访问的特性**,但也让很多操作变得低效,比如删除和插入

### 插入

有序:每个位置插入的时间复杂度是n

无序:只当作一个存储集合,可以插入到固定位置k,将k位置的元素放到最后,时间复杂度是1

### 删除

和插入类似

在一些特殊场景下,不一定追求连续性,将多次删除操作一起执行可以提高效率,比如只是记录数据已经被删除,在没有更多的空间时,一次性删除,也是JVM标记清除垃圾回收机制的核心思想

### 数组的访问越界

在C中是⼀种未决⾏为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问⼀段连续内存，只要数组通过偏移计算得到的内存地址是可⽤的，那么程序就可能不会报任何错误。

很多计算机病毒也正是利⽤到了代码中的数组越界可以访问⾮法地址的漏洞，来攻击系统，所以写代码的时候⼀定要警惕数组越界。

Java本身就会做越界检查，会抛出java.lang.ArrayIndexOutOfBoundsException。

### 容器

针对数组类型，很多语⾔都提供了容器类，⽐如Java中的ArrayList、C++ STL中的vector。

ArrayList最⼤的优势就是可以将很多数组操作的细节封装起来。⽐如前⾯提到的数组插⼊、删除数据时需要搬移其他数据等。另外，它还有⼀个优势，就是⽀持动态扩容。每次存储空间不够的时候，它都会将空间⾃动扩容为1.5倍⼤⼩。这⾥需要注意⼀点，因为扩容操作涉及内存申请和数据搬移，是⽐较耗时的。所以，最好在创建ArrayList的时候事先指定数据⼤⼩。

1.Java ArrayList⽆法存储基本类型，⽐如int、long，需要封装为Integer、Long类，⽽Autoboxing、Unboxing则有⼀定的性能消耗，所以如果特别关注性能，或者希望使⽤基本类型，就可以选⽤数组。

2.如果数据⼤⼩事先已知，并且对数据的操作⾮常简单，⽤不到ArrayList提供的⼤部分⽅法，也可以直接使⽤数组。

3.当要表示多维数组时，⽤数组往往会更加直观。⽐如Object[][] array；⽽⽤容器的话则需要这样定义：ArrayList<ArrayList > array。

**总结**：如果是做⼀些⾮常底层的开发，⽐如开发⽹络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为⾸选。

> 为什么⼤多数编程语⾔中，数组要从0开始编号，⽽不是从1开始呢？

从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前⾯也讲到，如果⽤a来表示数组的⾸地址，a[0]就是偏移为0的位置，也就是⾸地址

计算a[k]的内存地址只需要⽤这个公式：a[k]_address = base_address + (k-1)\*type_size

C语⾔设计者⽤0开始计数数组下标，之后的Java、JavaScript等⾼级语⾔都效仿了C语⾔，为了减少学习成本

## 链表

### LRU缓存淘汰算法

当缓存被⽤满时就需要缓存淘汰策略来决定。常⻅的有三种：先进先出策略FIFO（First In，First Out）、最少使⽤策略LFU（Least Frequently Used）、最近最少使⽤策略LRU（Least Recently Used）。

从底层的存储结构来看：它通过“指针”将⼀组零散的内存块串联起来使⽤，其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下⼀个结点的地址。这个记录下个结点地址的指针叫作后继指针next。

### 单链表

头结点⽤来记录链表的基地址。有了它，我们就可以遍历得到整条链表。

尾结点的指针不是指向下⼀个结点，⽽是指向⼀个空地址NULL，表示这是链表上最后⼀个结点。

**插入和删除**：

因为链表的存储空间本身就不是连续的。所以，在链表中插⼊和删除⼀个数据是⾮常快速的。对应的时间复杂度是O(1)。

随机访问：因为链表中的数据并⾮连续存储的，需要根据指针⼀个结点⼀个结点地依次遍历，直到找到相应的结点。

### 循环链表

⼀种特殊的单链表。它跟单链表唯⼀的区别就在尾结点。循环链表的尾结点指针是指向链表的头结点。它像⼀个环⼀样⾸尾相连，所以叫作“循环”链表。

循环链表的优点是从链尾到链头⽐较⽅便。当要处理的数据具有环型结构特点时，就特别适合采⽤循环链表。⽐如著名的约瑟夫问题。

### 双向链表

它⽀持两个⽅向，每个结点不⽌有⼀个后继指针next指向后⾯的结点，还有⼀个前驱指针prev指向前⾯的结点。

双向链表要⽐单链表占⽤更多的内存空间。虽然两个指针⽐较浪费存储空间，但可以⽀持双向遍历，这样也带来了双向链表操作的灵活性。

**插入和删除**：

删除结点中“值等于某个给定值”的结点：从头结点开始⼀个⼀个依次遍历对⽐，直到找到值等于给定值的结点，然后再指针操作将其删除。对应的时间复杂度为O(n)。

删除给定指针指向的结点：我们已经找到了要删除的结点，删除某个结点q需要知道其前驱结点，对于双向列表来说是已知的，所以时间复杂度为O(1)*

Java中的LinkedHashMap这个容器的实现原理，其中就⽤到了双向链表这种数据结构。

### 双向循环链表

### 数组和链表

两种截然不同的内存组织⽅式。正是因为内存存储的区别，它们插⼊、删除、随机访问操作的时间复杂度正好相反。

数组简单易⽤，在实现上使⽤的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以**访问效率更⾼**。

链表在内存中并不是连续存储，所以**对CPU缓存不友好，没办法有效预读**。

数组的缺点是**⼤⼩固定**，⼀经声明就要占⽤整块连续内存空间。如果声明的数组过⼤，可能导致“内存不⾜（out of memory）”。如果声明的数组过⼩，则可能出现不够⽤的情况。这时只能再申请⼀个更⼤的内存空间，把原数组拷⻉进去，⾮常费时。

链表本身没有⼤⼩的限制，天然地⽀持**动态扩容**，这也是它与数组最⼤的区别。

### 书写技巧

#### 理解引用或指针

有些语⾔有“指针”的概念，⽐如C语⾔；有些语⾔没有指针，取⽽代之的是“引⽤”，⽐如Java、Python。不管是“指针”还是“引⽤”，实际上，它们的意思都是⼀样的，都是存储所指对象的内存地址。

**将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。**

p->next=q：p结点中的next指针存储了q结点的内存地址。
p->next=p->next->next：p结点的next指针存储了p结点的下下⼀个结点的内存地址。

#### 警惕指针丢失和内存泄漏

我们希望在结点a和相邻的结点b之间插⼊结点x，假设当前指针p指向结点a。如果我们将代码实现变成下⾯这个样⼦，就会发⽣指针丢失和内存泄露。

```c
p->next = x; // 将p的next指针指向x结点；
x->next = p->next; // 相当于将x赋值给x->next，⾃⼰指向⾃⼰，整个链表也就断成了两半，从结点b往后的所有结点都⽆法访问到了。
```

对于有些语⾔来说，⽐如C语⾔，内存管理是由程序员负责的，如果没有⼿动释放结点对应的内存空间，就会产⽣内存泄露。所以，我们**插⼊结点时，⼀定要注意操作的顺序**，要先将结点x的next指针指向结点b，再把结点a的next指针指向结点x，这样才不会丢失指针，导致内存泄漏。所以，对于刚刚的插⼊代码，我们只需要把第1⾏和第2⾏代码的顺序颠倒⼀下就可以了。

#### 利用哨兵简化实现难度

针对链表的**插⼊、删除**操作，需要对插⼊第⼀个结点和删除**最后⼀个结点的情况进⾏特殊处理**。这样代码实现起来就会很繁琐，不简洁，⽽且也容易因为考虑不全⽽出错。哨兵，解决的是国家之间的边界问题。同理，这⾥说的哨兵也是解决“边界问题”的，不直接参与业务逻辑。

在任何时候，不管链表是不是空，head指针都会⼀直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫**带头链表**。相反，没有哨兵结点的链表就叫作**不带头链表**。你可以发现，哨兵结点是**不存储数据**的。因为哨兵结点⼀直存在，所以插⼊第⼀个结点和插⼊其他结点，删除最后⼀个结点和删除其他结点，都可以统⼀为相同的代码实现逻辑了。

这种利⽤哨兵简化编程难度的技巧，在很多代码实现中都有⽤到，⽐如插⼊排序、归并排序、动态规划等。

#### 留意边界条件处理

- 如果链表为空时，代码是否能正常⼯作？
- 如果链表只包含⼀个结点时，代码是否能正常⼯作？
- 如果链表只包含两个结点时，代码是否能正常⼯作？
- 代码逻辑在处理头结点和尾结点的时候，是否能正常⼯作？

针对不同的场景，可能还有特定的边界条件，

#### 举例画图

#### 多写多练

5个常⻅的链表操作

- 单链表反转
- 链表中环的检测
- 两个有序的链表合并
- 删除链表倒数第n个结点
- 求链表的中间结点

## 栈

### 理解栈

后进者先出，先进者后出，这就是典型的“栈”结构。

数组或链表暴露了太多的操作接⼝，操作上的确灵活⾃由，但使⽤时就⽐较不可控，⾃然也就更容易出错。当某个数据集合只涉及在⼀端插⼊和删除数据，并且满⾜后进先出、先进后出的特性，我们就应该⾸选“栈”这种数据结构。

### 实现栈

栈既可以⽤数组来实现，也可以⽤链表来实现。⽤数组实现的栈，我们叫作顺序栈，⽤链表实现的栈，我们叫作链式栈。

不管是顺序栈还是链式栈，我们存储数据只需要⼀个⼤⼩为n的数组就够了。在⼊栈和出栈过程中，只需要⼀两个临时变量存储空间，所以空间复杂度是O(1)。(我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运⾏还需要额外的存储空间。)

不管是顺序栈还是链式栈，⼊栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是O(1)。

#### 动态扩容的顺序栈

实现⼀个⽀持动态扩容的栈，我们只需要底层依赖⼀个⽀持动态扩容的数组就可以了。当栈满了之后，我们就申请⼀个更⼤的数组，将原来的数据搬移到新数组中。

以出栈的时间复杂度是O(1)，对于⼊栈操作来说，最好情况时间复杂度是O(1)，最坏情况时间复杂度是O(n)。均摊时间复杂度⼀般都等于最好情况时间复杂度。因为在⼤部分情况下，⼊栈操作的时间复杂度O都是O(1)，只有在个别时刻才会退化为O(n)，所以把耗时多的⼊栈操作的时间均摊到其他⼊栈操作上，平均情况下的耗时就接近O(1)。

### 栈的应用

#### 栈在函数调用中

操作系统给每个线程分配了⼀块独⽴的内存空间，这块内存被组织成“栈”这种结构,⽤来存储函数调⽤时的临时变量。每进⼊⼀个函数，就会将临时变量作为⼀个栈帧⼊栈，当被调⽤函数执⾏完成，返回之后，将这个函数对应的栈帧出栈。

#### 栈在表达式求值中

实际上，编译器就是通过两个栈来实现的。其中⼀个保存操作数的栈，另⼀个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压⼊操作数栈；当遇到运算符，就与运算符栈的栈顶元素进⾏⽐较。如果⽐运算符栈顶元素的优先级⾼，就将当前运算符压⼊栈；如果⽐运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取2个操作数，然后进⾏计算，再把计算完的结果压⼊操作数栈，继续⽐较。

#### 栈在括号匹配中

我们⽤栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压⼊栈中；当扫描到右括号时，从栈顶取出⼀个左括号。如果能够匹配，⽐如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为⾮法格式。当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为⾮法格式。

leedcode0020

#### 浏览器的前进后退

我们使⽤两个栈，X和Y，我们把⾸次浏览的⻚⾯依次压⼊栈X，当点击后退按钮时，再依次从栈X中出栈，并将出栈的数据依次放⼊栈Y。当我们点击前进按钮时，我们依次从栈Y中取出数据，放⼊栈X中。当栈X中没有数据时，那就说明没有⻚⾯可以继续后退浏览了。当栈Y中没有数据，那就说明没有⻚⾯可以点击前进按钮浏览了。

## 队列

### 理解队列

先进者先出，就是典型的“队列”。

栈只⽀持两个基本操作：⼊栈push()和出栈pop()。队列跟栈⾮常相似，⽀持的操作也很有限，最基本的操作也是两个：⼊队enqueue()，放⼀个数据到队列尾部；出队dequeue()，从队列头部取⼀个元素。所以，队列跟栈⼀样，也是⼀种操作受限的线性表数据结构。

### 实现队列

跟栈⼀样，队列可以⽤数组来实现，也可以⽤链表来实现。⽤数组实现的栈叫作顺序栈，⽤链表实现的栈叫作链式栈。同样，⽤数组实现的队列叫作顺序队列，⽤链表实现的队列叫作链式队列。

对于栈来说，我们只需要⼀个栈顶指针就可以了。但是队列需要两个指针：⼀个是head指针，指向队头；⼀个是tail指针，指向队尾。随着不停地进⾏⼊队、出队操作，head和tail都会持续往后移动。当tail移动到最右边，即使数组中还有空闲空间，也⽆法继续往队列中添加数据了。我们只需要在⼊队时，再集中触发⼀次数据的搬移操作。

### 循环队列

⽤数组来实现队列的时候，在tail==n时，会有数据搬移操作，这样⼊队操作性能就会受到影响。那有没有办法能够避免数据搬移呢？我们来看看循环队列的解决思路。

![循环队列-1](E:\Jennifer\other\notes\media\循环队列-1.png)

这个队列的⼤⼩为8，当前head=4，tail=7。当有⼀个新的元素a⼊队时，我们放⼊下标为7的位置。但这
个时候，我们并不把tail更新为8，⽽是将其在环中后移⼀位，到下标为0的位置。当再有⼀个元素b⼊队时，我们将b放⼊下标为0的位置，然后tail加1更新为1。

![循环队列-2](E:\Jennifer\other\notes\media\循环队列-2.png)

要想写出没有bug的循环队列的实现代码，最关键的是，确定好队空和队满的判定条件。队列为空的判断条件仍然是head == tail。但队列满的判断条件就稍微有点复杂了。

![循环队列-队满](E:\Jennifer\other\notes\media\循环队列-队满.png)

就像我图中画的队满的情况，tail=3，head=4，n=8，所以总结⼀下规律就是：(3+1)%8=4。多画⼏张队满的图，你就会发现，当队满时，(tail+1)%n=head。你有没有发现，当队列满时，图中的tail指向的位置实际上是没有存储数据的。所以，循环队列会浪费⼀个数组的存储空间。

### 阻塞队列和并发队列

队列这种数据结构很基础，平时的业务开发不⼤可能从零实现⼀个队列，甚⾄都不会直接⽤到。⽽⼀些具有特殊特性的队列应⽤却⽐较⼴泛，⽐如阻塞队列和并发队列。

**阻塞队列**其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插⼊数据的操作就会被阻塞，直到队列中有空闲位置后再插⼊数据，然后再返回。

上述的定义就是⼀个“⽣产者-消费者模型”！是的，我们可以使⽤阻塞队列，轻松实现⼀个“⽣产者-消费者
模型”！

这种基于阻塞队列实现的“⽣产者-消费者模型”，可以有效地协调⽣产和消费的速度。当“⽣产者”⽣产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，⽣产者就阻塞等待，直到“消费者”消费了数据，“⽣产者”才会被唤醒继续“⽣产”。⽽且不仅如此，基于阻塞队列，我们还可以通过协调“⽣产者”和“消费者”的个数，来提⾼数据的处理效率。⽐如前⾯的例⼦，我们可以多配置⼏个“消费者”，来应对⼀个“⽣产者”。

在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现⼀个线
程安全的队列呢？线程安全的队列我们叫作**并发队列**。最简单直接的实现⽅式是直接在enqueue()、dequeue()⽅法上加锁，但是锁粒度⼤并发度会⽐较低，同⼀时刻仅允许⼀个存或者取操作。实际上，基于数组的循环队列，利⽤CAS原⼦操作，可以实现⾮常⾼效的并发队列。这也是循环队列⽐链式队列应⽤更加⼴泛的原因。在实战篇讲Disruptor的时候，我会再详细讲并发队列的应⽤。

### 在线程池等有限资源池中的应用

CPU资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反⽽会导致CPU频繁切换，处理性能下降。所以，线程池的⼤⼩⼀般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。当我们向固定⼤⼩的线程池中请求⼀个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略⼜是怎么实现的呢？

我们⼀般有两种处理策略。第⼀种是⾮阻塞的处理⽅式，直接拒绝任务请求；另⼀种是阻塞的处理⽅式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。所以队列这种数据结构很适合来存储排队请求。

我们前⾯说过，队列有基于链表和基于数组这两种实现⽅式。基于链表的实现⽅式，可以实现⼀个⽀持⽆限排队的⽆界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过⻓。所以，针对响应时间⽐较敏感的系统，基于链表实现的⽆限排队的线程池是不合适的。

⽽基于数组实现的有界队列（bounded queue），队列的⼤⼩有限，所以线程池中排队的请求超过队列⼤⼩时，接下来的请求就会被拒绝，这种⽅式对响应时间敏感的系统来说，就相对更加合理。不过，设置⼀个合理的队列⼤⼩，也是⾮常有讲究的。队列太⼤导致等待的请求太多，队列太⼩会导致⽆法充分利⽤系统资源、发挥最⼤性能。

除了前⾯讲到队列应⽤在线程池请求排队的场景之外，队列可以应⽤在任何有限资源池中，⽤于排队请求，⽐如数据库连接池等。实际上，对于⼤部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

## 递归

### 理解递归

所有的递归问题都可以⽤递推公式来表示。

#### 满足递归的三个条件

1. ⼀个问题的解可以分解为⼏个⼦问题的解
2. 这个问题与分解之后的⼦问题，除了数据规模不同，求解思路完全⼀样
3. 存在递归终⽌条件

### 编写递归代码

写递归代码最关键的是写出递推公式，找到终⽌条件，剩下将递推公式转化为代码就很简单了。

假如这⾥有n个台阶，每次你可以跨1个台阶或者2个台阶，请问⾛这n个台阶有多少种⾛法？
我们仔细想下，实际上，可以根据第⼀步的⾛法把所有⾛法分为两类，第⼀类是第⼀步⾛了1个台阶，另⼀类是第⼀步⾛了2个台阶。所以n个台阶的⾛法就等于先⾛1阶后，n-1个台阶的⾛法 加上先⾛2阶后，n-2个台阶的⾛法。⽤公式表示就是：

```js
f(n) = f(n-1)+f(n-2)
```

当有⼀个台阶时，我们不需要再继续递归，就只有⼀种⾛法。所以f(1)=1。这个递归终⽌条件⾜够吗？我们可以⽤n=2，n=3这样⽐较⼩的数试验⼀下。n=2时，f(2)=f(1)+f(0)。如果递归终⽌条件只有⼀个f(1)=1，那f(2)就⽆法求解了。所以除了f(1)=1这⼀个递归终⽌条件外，还要有f(0)=1，表示⾛0个台阶有⼀种⾛法，不过这样⼦就不符合正常的逻辑思维了。所以，我们可以把f(2)=2作为⼀种终⽌条件，表示⾛2个台阶，有两种⾛法，⼀步⾛完或者分两步来⾛。所以，递归终⽌条件就是f(1)=1，f(2)=2。这个时候，你可以再拿n=3，n=4来验证⼀下，这个终⽌条件是否⾜够并且正确。我们把递归终⽌条件和刚刚得到的递推公式放到⼀起就是这样的：

```js
f(1) = 1;
f(2) = 2;
f(n) = f(n-1)+f(n-2)
```

写递归代码的关键就是找到如何将⼤问题分解为⼩问题的规律，并且基于此写出递推公式，然后再推敲终⽌条件，最后将递推公式和终⽌条件翻译成代码。

对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进⼊了⼀个思维误区。很多时候，我们理解起来⽐较吃⼒，主要原因就是⾃⼰给⾃⼰制造了这种理解障碍。那正确的思维⽅式应该是怎样的呢？
如果⼀个问题A可以分解为若⼲⼦问题B、C、D，你可以假设⼦问题B、C、D已经解决，在此基础上思考如何解决问题A。⽽且，你只需要思考问题A与⼦问题B、C、D两层之间的关系即可，不需要⼀层⼀层往下思考⼦问题与⼦⼦问题，⼦⼦问题与⼦⼦⼦问题之间的关系。屏蔽掉递归细节，这样⼦理解起来就简单多了。因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成⼀个递推公式，不⽤想⼀层层的调⽤关系，不要试图⽤⼈脑去分解递归的每个步骤。

### 警惕堆栈溢出

我们可以通过在代码中限制递归调⽤的最⼤深度的⽅式来解决这个问题。递归调⽤超过⼀定深度（⽐如1000）之后，我们就不继续往下再递归了，直接返回报错。

但这种做法并不能完全解决问题，因为最⼤允许的递归深度跟当前线程剩余的栈空间⼤⼩有关，事先⽆法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。所以，如果最⼤深度⽐较⼩，⽐如10、50，就可以⽤这种⽅法，否则这种⽅法并不是很实⽤。

### 警惕重复计算

想要计算f(5)，需要先计算f(4)和f(3)，⽽计算f(4)还需要计算f(3)，因此，f(3)就被计算了很多次，这就是重复计算问题。为了避免重复计算，我们可以通过⼀个数据结构（⽐如散列表）来保存已经求解过的f(k)。当递归调⽤到f(k)时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。

### 其他问题

在时间效率上，递归代码⾥多了很多函数调⽤，当这些函数调⽤的数量较⼤时，就会积聚成⼀个可观的时间成本。在空间复杂度上，因为递归调⽤⼀次就会在内存栈中保存⼀次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，空间复杂度往往并不是O(1)，⽽是O(n)。

如果数据库⾥存在脏数据，我们还需要处理由此产⽣的⽆限递归问题。⽐如demo环境下数据库中，测试⼯程师为了⽅便测试，会⼈为地插⼊⼀些数据，就会出现脏数据。可以⽤限制递归深度来解决。不过，还有⼀个更⾼级的处理⽅法，就是⾃动检测A-B-C-A这种“环”的存在。

### 改为非递归

递归有利有弊，利是递归代码的表达⼒很强，写起来⾮常简洁；⽽弊就是空间复杂度⾼、有堆栈溢出的⻛险、存在重复计算、过多的函数调⽤会耗时较多等问题。所以，在开发过程中，我们要根据实际情况来选择是否需要⽤递归的⽅式来实现。

笼统地讲，所有的递归代码都可以改为迭代循环的⾮递归写法。因为递归本身就是借助栈来实现的，只不过我们使⽤的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们⾃⼰在内存堆上实现栈，⼿动模拟⼊栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样⼦。

但是这种思路实际上是将递归改为了“⼿动”递归，本质并没有变，⽽且也并没有解决前⾯讲到的某些问题，徒增了实现的复杂度。

# 算法

## 排序

### 分析排序算法

#### 执行效率

1. 最好情况、最坏情况、平均情况时间复杂度
2. 时间复杂度的系数、常数 、低阶
3. ⽐较次数和交换（或移动）次数

#### 内存消耗

耗可以通过空间复杂度来衡量，原地排序（Sorted in place），就是特指空间复杂度是O(1)的排序算法。下面的三种排序算法，都是原地排序算法。

#### 稳定性

稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。比如，我们希望按照⾦额从⼩到⼤对订单数据排序。对于⾦额相同的订单，我们希望按照下单时间从早到晚有序。

### 冒泡排序

冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进⾏⽐较，看是否满⾜⼤⼩关系要求。如果不满⾜就让它俩互换。⼀次冒泡会让⾄少⼀个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序⼯作。的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，？？？说明已经达到完全有序，不⽤再继续执⾏后续的冒泡操作。我这⾥还有另外⼀个例⼦，这⾥⾯给6个元素排序，只需要4次冒泡操作就可以了。

有序度是数组中具有有序关系的元素对的个数。种完全有序的数组的有序度叫作满有序度。**逆序度=满有序度-有序度**，我们排序的过程就是⼀种增加有序度，减少逆序度的过程，最后达到满有序度，就说明排序完成了。

1. 冒泡排序是**原地排序算法**，冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为O(1)，是⼀个原地排序算法。
2. 冒泡排序是**稳定的排序算法**吗，当有相邻的两个元素⼤⼩相等的时候，我们不做交换，相同⼤⼩的数据在排序前后不会改变顺序。
3. 冒泡排序的**时间复杂度**：最好情况下，我们只需要进⾏⼀次冒泡操作，所以最好情况时间复杂度是**O(n)**。⽽最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进⾏n次冒泡操作，所以最坏情况时间复杂度为**O(n2)**。

```js
// 交换顺序
function swap(arr, a, b) {
    if (!arr instanceof Array) return
    [arr[a], arr[b]] = [arr[b], arr[a]]
}
// 比较相邻的两个元素
let m = 0
let n = 0
let s = 0
// arr = [4,3,2,1]
let arr = [1,2,4,3,5,7,9]
function bubble(arr) {
    if(!arr instanceof Array) return
    for(let i = 0; i < arr.length; i++) {
        m++
        let f = false
        for(let j = 0; j < arr.length - (i + 1); j++) {
            n++
            if(arr[j] > arr[j+1]) {
                s++
                f = true
                swap(arr, j, j+1)
            }
        }
        if(!f) return arr
    }
    // return arr
}

console.log('bubble2', bubble2(arr))
console.log(m)
console.log(n)
console.log(s)
```

### 插入排序

我们将数组中的数据分为两个区间，已排序区间和未排序区间。初始已排序区间只有⼀个元素，就是数组的第⼀个元素。插⼊算法的核⼼思想是取未排序区间中的元素，在已排序区间中找到合适的插⼊位置将其插⼊，并保证已排序区间数据⼀直有序。重复这个过程，直到未排序区间中元素为空，算法结束。

移动次数就等于逆序度

1. 插⼊排序是**原地排序算法**
2. 插⼊排序是**稳定的排序算法**
3. 插⼊排序的**平均时间复杂度为O(n2)**

```js
let m = 0
let n = 0
let s = 0
let arr = [2,1,3,4,5,7,9]

function insert(arr) {
    if (!arr instanceof Array) return
    for (let i = 1; i < arr.length; i++) {
        let now = arr[i]
        m++
        let j = i - 1
        for (; j >= 0; --j) {
            n++
            // 比较当前元素和之前的元素，小时交换
            if (arr[j] > now) {
                s++
                arr[j+1] = arr[j]
            } else {
                break
            }
        }
        arr[j+1] = now
    }
    return arr
}

console.log('arr', arr)
console.log('insert', insert(arr))
console.log(m)
console.log(n)
console.log(s)
```

**为什么插⼊排序要⽐冒泡排序更受欢迎呢？**
冒泡排序不管怎么优化，元素交换的次数是⼀个固定值，是原始数据的逆序度。插⼊排序是同样的，不管怎么优化，元素移动的次数也等于原始数据的逆序度。但是，从代码实现上来看，冒泡排序的数据交换要⽐插⼊排序的数据移动要复杂，冒泡排序需要3个赋值操作，⽽插⼊排序只需要1个。所以，虽然冒泡排序和插⼊排序在时间复杂度上是⼀样的，都是O(n2)，但是如果我们希望把性能优化做到极致，那肯定⾸选
插⼊排序。插⼊排序的算法思路也有很⼤的优化空间，我们只是讲了最基础的⼀种。可以了解希尔排序。

### 选择排序

选择排序算法的实现思路有点类似插⼊排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最⼩的元素，将其放到已排序区间的末尾。

1. 选择排序空间复杂度为O(1)，是⼀种**原地排序算法**。

2. 选择排序的最好情况时间复杂度、最坏情况和平均情况**时间复杂度都为O(n2)**。

3. 选择排序是⼀种**不稳定的排序算法**。选择排序每次都要找剩余未排序元素中的最⼩值，并和前⾯的元素交换位置，这样破坏了稳定性。

### 归并排序

如果要排序⼀个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在⼀起，这样整个数组就都有序了。归并排序使⽤的就是**分治思想**。将⼀个⼤问题分解成⼩的⼦问题来解决。⼩的⼦问题解决了，⼤问题也就解决了。

```js
function mergeSort(arr) {
    // 结束条件
    if(arr.length <= 1) return arr
    // 取中间索引
    let mid = ~~(arr.length / 2)
    let left = arr.slice(0, mid)
    let right = arr.slice(mid)
    return merge(mergeSort(left), mergeSort(right))
}

function merge(left, right) {
    let tmp = []

    while(left.length && right.length) {
        // 排序两个有序数组
        // 找出最小值插入tmp
        if(left[0] < right[0]) {
            tmp.push(right.shift())
        }else {
            tmp.push(left.shift())
        }
    }
    // 此时有一变为空，合并剩下的数组
    return tmp.concat(left, right)
}
```

分治算法⼀般都是⽤递归来实现的。分治是⼀种解决问题的处理思想，递归是⼀种编程技巧。

1. 归并排序是⼀种**稳定的排序算法**。先把left中的元素放⼊tmp数组。这样就保证了值相同的元素，在合并前后的先后顺序不变。

2. 归并排序的最好情况、最坏情况，还是平均情况，**时间复杂度都是O(nlogn)**。

3. 尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU只会有⼀个函数在执⾏，也就只会有⼀个临时的内存空间在使⽤。临时内存空间最⼤也不会超过n个数据的⼤⼩，所以**空间复杂度是O(n)**。

### 快速排序

我们选择p到r之间的任意⼀个数据作为pivot（分区点）。我们遍历p到r之间的数据，将⼩于pivot的放到左边，将⼤于pivot的放到右边，将pivot放到中间。经过这⼀步骤之后，数组p到r之间的数据就被分成了三个部分，前⾯p到q-1之间都是⼩于pivot的，中间是pivot，后⾯的q+1到r之间是⼤于pivot的。

分区函数原理

![快排-分区函数](E:\Jennifer\other\notes\media\快排-分区函数.png)

```js
function quickSort(arr) {
    if(arr.length <= 1) return arr

    // 选择基准值
    let midItem = arr.splice(0, 1)[0]
    let left = []
    let right = []
    arr.forEach(item => {
        if(item <= midItem) {
            left.push(item)
        }else {
            right.push(item)
        }
    });
    return quickSort(left).concat(midItem, quickSort(right))
}

const testArr = []
let i = 0
while (i < 1000) {
    testArr.push(Math.floor(Math.random() * 1000))
    i++
}

const res = quickSort(testArr)
console.log(res)

// 可以优化成原地排序

// 交换顺序
function swap(arr, a, b) {
    if (!arr instanceof Array) return
    [arr[a], arr[b]] = [arr[b], arr[a]]
}

function quickSort2(arr, s, e) {
    if(arr.length <= 1) return arr
    let midItem = arr.splice(s, 1)[0]
    
    // 原地分区
    let p = partition(arr, s, e)
    return quickSort(arr, s, p).concat(midItem, quickSort(arr, p, e))
}

// 分区函数
function partition(arr, s, e) {
    let i = s
    for(let j=i; j<e; j++) {
        if(arr[j] > arr[i]) {
            swap(arr, i, j)
            i++
        }
    }
    return i
}

const res2 = quickSort2(testArr, 0, testArr.length)
console.log(res2)
```

#### 性能分析

1. 因为分区的过程涉及交换操作，如果数组中有两个相同的元素，在经过第⼀次分区操作之后，两个元素的相对先后顺序就会改变。所以，快速排序并**不是⼀个稳定的排序算法**。

2. 如果每次分区操作，都能正好把数组分成⼤⼩接近相等的两个⼩区间，那快排的时间复杂度递推求解公式跟归并是相同的。所以，快排的时间复杂度也是**O(nlogn)**。如果数组中的数据原来已经是有序的了。如果我们每次选择最后⼀个元素作为pivot，那每次分区得到的两个区间都是不均等的。我们需要进⾏⼤约n次分区操作，才能完成快排的整个过程。每次分区我们平均要扫描⼤约n/2个元素，这种情况下，快排的时间复杂度就从O(nlogn)**退化成了O(n2)**。

3. 快速排序通过设计巧妙的原地分区函数，可以实现原地排序，所以**空间复杂度是O(1)**。

#### 对比归并排序

可以发现，归并排序的处理过程是由下到上的，先处理⼦问题，然后再合并。⽽快排正好相反，它的处理过程是**由上到下**的，先分区，然后再处理⼦问题。归并排序虽然是稳定的、时间复杂度为O(nlogn)的排序算法，但是它是⾮原地排序算法。我们前⾯讲过，归并之所以是⾮原地排序算法，主要原因是合并函数⽆法在原地执⾏。快速排序通过设计巧妙的原地分区函数，**可以实现原地排序**，解决了归并排序占⽤太多内存的问题。

### 线性排序

**复杂度是O(n)**的排序算法叫作线性排序（Linear sort）比如：**桶排序、计数排序、基数排序**。之所以能做到线性的时间复杂度，主要原因是，这三个算法是**⾮基于⽐较的排序算法**，都不涉及元素之间的⽐较操作。这⼏种排序算法对要排序的数据要求很苛刻，所以重点的是掌握这些排序算法的适⽤场景。

#### 桶排序（Bucket sort）

核⼼思想是将要排序的数据分到⼏个有序的桶⾥，每个桶⾥的数据再单独进⾏排序。桶内排完序之后，再把每个桶⾥的数据按照顺序依次取出，组成的序列就是有序的了。

如果要排序的数据有n个，我们把它们均匀地划分到m个桶内，每个桶⾥就有k=n/m个元素。每个桶内部使⽤快速排序，整个桶排序的时间复杂度就是O(n*log(n/m))。**当桶的个数m接近数据个数n时**，桶排序的时间复杂度接近O(n)。

**适用条件：**

⾸先，要排序的数据需要**很容易就能划分成m个桶**，并且，**桶与桶之间有着天然的⼤⼩顺序**。这样桶与桶之间的数据不需要再进⾏排序。

其次，数据在**各个桶之间的分布是⽐较均匀**的。如果数据经过桶的划分之后，桶⾥的数据很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到⼀个桶⾥，那就退化为O(nlogn)的排序算法了。

桶排序⽐较**适合⽤在外部排序**中。所谓的外部排序就是数据存储在外部磁盘中，数据量⽐较⼤，内存有限，⽆法将数据全部加载到内存中。

**举例：**

⽐如说我们有10GB的订单数据，我们希望按订单⾦额（假设⾦额都是正整数）进⾏排序，但是我们的内存有限，只有⼏百MB，没办法⼀次性把10GB的数据都加载到内存中。这个时候该怎么办呢？

我们可以先扫描⼀遍⽂件，看订单⾦额所处的数据范围。假设订单⾦额最⼩是1元，最⼤是10万元。我们将所有订单根据⾦额划分到100个桶⾥，每⼀个桶对应⼀个⽂件，并且按照⾦额范围的⼤⼩顺序编号命名（00，01，02…99）。理想的情况下，如果订单⾦额在1到10万之间均匀分布，每个⼩⽂件中存储⼤约100MB的订单数据，我们就可以将这100个⼩⽂件依次放到内存中，⽤快排来排序。等所有⽂件都排好序之后，我们只需要按照⽂件编号，从⼩到⼤依次读取每个⼩⽂件中的订单数据，并将其写⼊到⼀个⽂件中，那这个⽂件中存储的就是按照⾦额从⼩到⼤排序的订单数据了。但是订单按照⾦额在1元到10万元之间并不⼀定是均匀分布的 ，所以10GB订单数据是⽆法均匀地被划分到100个⽂件中的。针对这些划分之后还是⽐较⼤的⽂件，我们可以继续划分，⽐如，订单⾦额在1元到1000元之间的⽐较多，我们就将这个区间继续划分为10个⼩区间，1元到100元，101元到200元，201元到300元…901元到1000元。如果划分之后，101元到200元之间的订单还是太多，那就继续再划分，直到所有的⽂件都能读⼊内存为⽌。

#### 计数排序（Counting sort）

计数排序其实是桶排序的⼀种特殊情况。当要排序的n个数据，所处的**范围并不⼤**的时候，⽐如最⼤值是k，我们就可以把数据划分成k个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。

![计数排序](E:\Jennifer\other\notes\media\计数排序.png)

使用了利⽤另外⼀个数组来计数的实现⽅式，这也是为什么这种排序算法叫计数排序的原因。

计数排序只能⽤在数据范围不⼤的场景中，如果数据范围k⽐要排序的数据n⼤很多，就不适合⽤计数排序了。⽽且，计数排序只能给⾮负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对⼤⼩的情况下，转化为⾮负整数。

#### 基数排序（Radix sort）

假设要⽐较两个⼿机号码a，b的⼤⼩，如果在前⾯⼏位中，a⼿机号码已经⽐b⼿机号码⼤了，那后⾯的⼏位就不⽤看了。

借助稳定排序算法，这⾥有⼀个巧妙的实现思路。先按照最后⼀位来排序⼿机号码，然后，再按照倒数第⼆位重新排序，以此类推，最后按照第⼀位重新排序。经过11次排序之后，⼿机号码就都有序了。

![基数排序](E:\Jennifer\other\notes\media\基数排序.png)

注意，这⾥按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的。因为如果是⾮稳定排序算法，那最后⼀次排序只会考虑最⾼位的⼤⼩顺序，完全不管其他位的⼤⼩关系，那么低位的排序就完全没有意义了。

根据每⼀位来排序，我们可以⽤刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到O(n)。如果要排序的数据有k位，那我们就需要k次桶排序或者计数排序，总的时间复杂度是O(k*n)。当k不⼤的时候，⽐如⼿机号码排序的例⼦，k最⼤就是11，所以基数排序的时间复杂度就近似于O(n)

有时候要排序的数据并不都是等⻓的，比如单词，我们可以把所有的单词补⻬到相同⻓度，位数不够的可以在后⾯补“0”，因为根据ASCII值，所有字⺟都⼤于“0”，所以补“0”不会影响到原有的⼤⼩顺序。这样就可以继续⽤基数排序了。

基数排序对要排序的数据是有要求的，需要可以分割出独⽴的“位”来⽐较，⽽且位之间有递进的关系，如果a数据的⾼位⽐b数据⼤，那剩下的低位就不⽤⽐较了。除此之外，每⼀位的数据范围不能太⼤，要可以⽤线性排序算法来排序，否则，基数排序的时间复杂度就⽆法做到O(n)了。

### ⼩结

桶排序和计数排序的排序思想是⾮常相似的，都是针对范围不⼤的数据，将数据划分成不同的桶来实现排序。基数排序要求数据可以划分成⾼低位，位之间有递进关系。⽐较两个数，我们只需要⽐较⾼位，⾼位相同的再⽐较低位。⽽且每⼀位的数据范围不能太⼤，因为基数排序算法需要借助桶排序或者计数排序来完成每⼀个位的排序⼯作。

### 排序优化

![排序对比](E:\Jennifer\other\notes\media\排序对比.png)

线性排序算法的时间复杂度⽐较低，适⽤场景⽐较特殊。所以如果要写⼀个通⽤的排序函数，不能选择线性排
序算法。如果对⼩规模数据进⾏排序，可以选择时间复杂度是O(n2)的算法；如果对⼤规模数据进⾏排序，时间复杂度是O(nlogn)的算法更加⾼效。所以，为了兼顾任意规模数据的排序，⼀般都会⾸选时间复杂度是O(nlogn)的排序算法来实现排序函数。

归并排序并不是原地排序算法，空间复杂度是O(n)。除了数据本身占⽤的内存之外，排序算法还要额外再占⽤内存空间，空间耗费就翻倍了。所以，快速排序⽐较适合来实现排序函数，但是，我们也知道，快速排序在最坏情况下的时间复杂度是O(n )，如何来解决这个“复杂度恶化”的问题呢？

#### 优化快排

如果数据原来就是有序的或者接近有序的，每次分区点都选择最后⼀个数据，那快速排序算法就会变得⾮常糟糕，时间复杂度就会退化为O(n2)。实际上，这种O(n2)时间复杂度出现的**主要原因还是因为我们分区点选的不够合理**。

最理想的分区点是：**被分区点分开的两个分区中，数据的数量差不多**。

1. **三数取中法**
   我们从区间的⾸、尾、中间，分别取出⼀个数，然后对⽐⼤⼩，取这3个数的中间值作为分区点。这样每间隔某个固定的⻓度，取数据出来⽐较，将中间值作为分区点的分区算法，肯定要⽐单纯取某⼀个数据更好。但是，如果要排序的数组⽐较⼤，那“三数取中”可能就不够了，可能要“五数取中”或者“⼗数取中”。
2. **随机法**
   随机法就是每次从要排序的区间中，随机选择⼀个元素作为分区点。从概率的⻆度来看，不⼤可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是⽐较好的。时间复杂度退化为最糟糕的O(n2)的情况，出现的可能性不⼤。

快速排序是⽤递归来实现的。为了避免快速排序⾥，递归过深⽽堆栈过⼩，导致堆栈溢出，我们有两种解决办法：第⼀种是限制递归深度。⼀旦递归过深，超过了我们事先设定的阈值，就停⽌递归。第⼆种是通过在堆上模拟实现⼀个函数调⽤栈，⼿动模拟递归压栈、出栈的过程，这样就没有了系统栈⼤⼩的限制。

#### 分析排序函数

拿Glibc中的qsort()函数举例说明⼀下。它并不仅仅⽤了快排这⼀种算法。

qsort()会优先使⽤归并排序来排序输⼊数据，因为归并排序的空间复杂度是O(n)，所以对于⼩数据量的排序，⽐如1KB、2KB等，归并排序额外需要1KB、2KB的内存空间，这个问题不⼤。但如果数据量太⼤，就跟我们前⾯提到的，排序100MB的数据，这个时候我们再⽤归并排序就不合适了。所以，要排序的数据量⽐较⼤的时候，qsort()会改为⽤快速排序算法来排序。qsort()选择分区点的⽅法就是“三数取中法”。递归太深会导致堆栈溢出的问题，qsort()是通过⾃⼰实现⼀个堆上的栈，⼿动模拟递归来解决的。

实际上，qsort()并不仅仅⽤到了归并排序和快速排序，它还⽤到了插⼊排序。在快速排序的过程中，当要排序的区间中，元素的个数⼩于等于4时，qsort()就退化为插⼊排序，不再继续⽤递归来做快速排序，因为我们前⾯也讲过，在⼩规模数据⾯前，O(n2)时间复杂度的算法并不⼀定⽐O(nlogn)的算法执⾏时间⻓。

算法的性能可以通过时间复杂度来分析，但是，这种复杂度分析是⽐较偏理论的，如果我们深究的话，实际上时间复杂度并不等于代码实际的运⾏时间。时间复杂度代表的是⼀个增⻓趋势，如果画成增⻓曲线图，你会发现O(n2)⽐O(nlogn)要陡峭，也就是说增⻓趋势要更猛⼀些。但是，我们前⾯讲过，在⼤O复杂度表示法中，我们会省略低阶、系数和常数，也就是说，O(nlogn)在没有省略低阶、系数、常数之前可能是O(knlogn + c)，⽽且k和c有可能还是⼀个⽐较⼤的数。

所以，对于⼩规模数据的排序，O(n )的排序算法并不⼀定⽐O(nlogn)排序算法执⾏的时间⻓。对于⼩数据量的排序，我们选择⽐较简单、不需要递归的插⼊排序算法。

还记得我们之前讲到的哨兵来简化代码，提⾼执⾏效率吗？在qsort()插⼊排序的算法实现中，也利⽤了这种编程技巧。虽然哨兵可能只是少做⼀次判断，但是毕竟排序函数是⾮常常⽤、⾮常基础的函数，性能的优化要做到极致。

## 二分查找

⼀种**针对有序数据集合的查找算法**（Binary Search），也叫折半查找算法。时间复杂度就是O(logn)。由于⽤⼤O标记法表示时间复杂度的时候，会省略掉常数、系数和低阶。O(1)有可能表示的是⼀个⾮常⼤的常量值。所以，常量级时间复杂度的算法有时候可能还没有O(logn)的算法执⾏效率⾼。

### 实现

最简单的情况就是有序数组中不存在重复元素，

1. 循环退出条件

2. mid的取值

3. low和high的更新

可以用递归和循环实现

### 应用场景的局限性

⾸先，⼆分查找**依赖的是顺序表结构**，简单点说就是数组。主要原因是⼆分查找算法需要按照下标随机访问元素。，数组按照下标随机访问数据的时间复杂度是O(1)，⽽链表随机访问的时间复杂度是O(n)。所以，如果数据使⽤链表存储，⼆分查找的时间复杂就会变得很⾼。

其次，⼆分查找**针对的是有序数据**。⼆分查找只能⽤在插⼊、删除操作不频繁，⼀次排序多次查找的场景中。针对动态变化的数据集合，⼆分查找将不再适⽤。维护有序的成本都是很⾼的。

再次，**数据量太⼩不适合⼆分查找**。顺序遍历就⾜够了。这⾥有⼀个例外。如果数据之间的⽐较操作⾮常耗时，不管数据量⼤⼩，我都推荐使⽤⼆分查找。⽐如，数组中存储的都是⻓度超过300的字符串。

最后，**数据量太⼤也不适合⼆分查找**。底层需要依赖数组这种数据结构，⽽数组为了⽀持随机访问的特性，要求内存空间连续。

道散列表、⼆叉树这些是⽀持快速查找的动态数据结构。⼤部分情况下，⽤⼆分查找可以解决的问题，⽤散列表、⼆叉树都可以解决。但是，不管是散列表还是⼆叉树，都会需要⽐较多的额外的内存空间。⽽⼆分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储⽅式。

### 二分查找的变体

#### 查找第⼀个等于给定值的元素

#### 查找最后⼀个等于给定值的元素

#### 查找第⼀个⼤于等于给定值的元素

#### 查找最后⼀个⼩于等于给定值的元素

凡是⽤⼆分查找能解决的，绝⼤部分我们更倾向于⽤散列表或者⼆叉查找树。即便是⼆分查找在内存使⽤上更节省，但是毕竟内存如此紧缺的情况并不多。实际上，上⼀节讲的求“值等于给定值”的⼆分查找确实不怎么会被⽤到，⼆分查找**更适合⽤在“近似”查找问题**，在这类问题上，⼆分查找的优势更加明显。⽐如今天讲的这⼏种变体问题，⽤其他数据结构，⽐如散列表、⼆叉树，就⽐较难实现了。变体的⼆分查找算法写起来⾮常烧脑，很容易因为细节处理不好⽽产⽣Bug，这些容易出错的细节有：终⽌条件、区间上下界更新⽅法、返回值选择。

## 跳表

如果数据存储在链表中，实际上，我们只需要对链表稍加改造，就可以⽀持类似“⼆分”的查找算法。我们把改造之后的数据结构叫作跳表（Skiplist）。

是它确实是⼀种各⽅⾯性能都⽐较优秀的动态数据结构，可以⽀持快速的插⼊、删除、查找操作，写起来也不复杂，甚⾄可以替代红⿊树（Red-blacktree）。

### 理解跳表

对链表建⽴⼀级“索引”，查找起来是不是就会更快⼀些呢？每两个结点提取⼀个结点到上⼀级，我们把抽出来的那⼀级叫作索引或索引层。图中的down表示down指针，指向下⼀级结点。这种**链表加多级索引**的结构，就是跳表。

![跳表](E:\Jennifer\other\notes\media\跳表.png)

如果每⼀层都要遍历m个结点，那在跳表中查询⼀个数据的**时间复杂度就是O(m*logn)**。我们每⼀级索引都最多只需要遍历3个结点，也就是说m=3。

跳表的**空间复杂度是O(n)**。在实际的软件开发中，原始链表中存储的有可能是很⼤的对象，⽽索引结点只需要存储关键值和⼏个指针，并不需要存储对象，所以当对象⽐索引结点⼤很多时，那索引占⽤的额外空间就可以忽略了。

### ⾼效的动态插⼊和删除

### 索引动态更新

作为⼀种动态数据结构，我们需要某种⼿段来维护索引与原始链表⼤⼩之间的平衡，也就是说，如果链表中结点多了，索引结点就相应地增加⼀些，避免复杂度退化，以及查找、插⼊、删除操作性能下降。

如果你了解红⿊树、AVL树这样平衡⼆叉树，你就知道它们是通过左右旋的⽅式保持左右⼦树的⼤⼩平衡，⽽跳表是通过随机函数来维护前⾯提到的“平衡性”。

### Redis对跳表的应用

Redis中的有序集合⽀持的核⼼操作主要有下⾯这⼏个：

- 插⼊⼀个数据；

- 删除⼀个数据；

- 查找⼀个数据；

- 按照区间查找数据（⽐如查找值在[100, 356]之间的数据）；

- 迭代输出有序序列。

其中，插⼊、删除、查找以及迭代输出有序序列这⼏个操作，红⿊树也可以完成，时间复杂度跟跳表是⼀样的。但是，按照区间来查找数据这个操作，红⿊树的效率没有跳表⾼。

Redis之所以⽤跳表来实现有序集合，还有其他原因，⽐如，跳表更容易代码实现。虽然跳表的实现也不简单，但⽐起
红⿊树来说还是好懂、好写多了，⽽简单就意味着可读性好，不容易出错。还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执⾏效率和内存消耗。

不过，跳表也不能完全替代红⿊树。因为红⿊树⽐跳表的出现要早⼀些，很多编程语⾔中的Map类型都是通过红⿊树来实现的。我们做业务开发的时候，直接拿来⽤就可以了，不⽤费劲⾃⼰去实现⼀个红⿊树，但是跳表并没有⼀个现成的实现，所以在开发中，如果你想使⽤跳表，必须要⾃⼰实现。

## 散列表

### 散列思想

散列表⽤的是数组⽀持按照下标随机访问数据的特性，所以散列表其实就是数组的⼀种扩展，由数组演化⽽来。

我们把键转化为数组下标的映射⽅法就叫作散列函数（或“Hash函数”“哈希函数”），⽽散列函数计算得到的值就叫作散列值（或“Hash值”“哈希值”）。

散列表⽤的就是数组⽀持按照下标随机访问的时候，时间复杂度是O(1)的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们⽤同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

### 散列函数

散列函数设计的基本要求：
1. 散列函数计算得到的散列值是⼀个⾮负整数；
2. 如果key1 = key2，那hash(key1) == hash(key2)；
3. 如果key1 ≠ key2，那hash(key1) ≠ hash(key2)。

要想找到⼀个不同的key对应的散列值都不⼀样的散列函数，⼏乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也⽆法完全避免这种散列冲突。⽽且，因为数组的存储空间有限，也会加⼤散列冲突的概率。所以我们⼏乎⽆法找到⼀个完美的⽆冲突的散列函数，即便能找到，付出的时间成本、计算成本也是很⼤的，所以针对散列冲突问题，我们需要通过其他途径来解决。

### 散列冲突

#### 开放寻址法

开放寻址法的核⼼思想是，如果出现了散列冲突，我们就重新探测⼀个空闲位置，将其插⼊。⼀个⽐较简单的探测⽅法就是**线性探测（Linear Probing）**。

当我们往散列表中插⼊数据时，如果某个数据经过散列函数散列之后，存储位置已经被占⽤了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为⽌。

在散列表中查找元素的过程有点⼉类似插⼊过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后⽐较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。

![在散列表中查找](E:\Jennifer\other\notes\media\在散列表中查找.png)

散列表跟数组⼀样，不仅⽀持插⼊、查找操作，还⽀持删除操作。对于使⽤线性探测法解决冲突的散列表，删除操作稍微有些特别。我们不能单纯地把要删除的元素设置为空。在查找的时候，⼀旦我们通过线性探测⽅法，找到⼀个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。

我们可以将删除的元素，特殊标记为deleted。当线性探测查找的时候，遇到标记为deleted的空间，并不是停下来，⽽是继续往下探测。

线性探测法其实存在很⼤问题。当散列表中插⼊的数据越来越多时，散列冲突发⽣的可能性就会越来越
⼤，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。

**⼆次探测**

跟线性探测很像，线性探测每次探测的步⻓是1，那它探测的下标序列就是hash(key)+0，hash(key)+1，hash(key)+2……⽽⼆次探测探测的步⻓就变成了原来的“⼆次⽅”，也就是说，它探测的下标序列就是hash(key)+0，hash(key)+1(平方)，hash(key)+2(平方) ……

**双重散列**

意思就是不仅要使⽤⼀个散列函数。我们使⽤⼀组散列函数hash1(key)，hash2(key)，hash3(key)……我们先⽤第⼀个散列函数，如果计算得到的存储位置已经被占⽤，再⽤第⼆个散列函数，依次类推，直到找到空闲的存储位置。

不管采⽤哪种探测⽅法，当散列表中空闲位置不多的时候，散列冲突的概率就会⼤⼤提⾼。为了尽可能保证散列表的操作效率，⼀般情况下，我们会尽可能保证散列表中有⼀定⽐例的空闲槽位。我们⽤**装载因⼦（load factor）**来表示空位的多少。

`散列表的装载因⼦=填⼊表中的元素个数/散列表的⻓度`

装载因⼦越⼤，说明空闲位置越少，冲突越多，散列表的性能会下降。

#### 链表法

在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应⼀条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。

![链表法](E:\Jennifer\other\notes\media\链表法.png)

当插⼊的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插⼊到对应链表中即可，所以插⼊的时间复杂度是O(1)。当查找、删除⼀个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。这两个操作的时间复杂度跟链表的⻓度k成正⽐，也就是O(k)。对于散列⽐较均匀的散列函数来说，理论上讲，k=n/m，其中n表示散列中数据的个数，m表示散列表中“槽”的个数。

散列表两个核⼼问题是散列函数设计和散列冲突解决。散列冲突有两种常⽤的解决⽅法，开放寻址法和链表法。散列函数设计的好坏决定了散列冲突的概率，也就决定散列表的性能。











































