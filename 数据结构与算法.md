# 概念
从⼴义上讲，数据结构就是指⼀组数据的存储结构。算法就是操作数据的⼀组⽅法。
从狭义上讲，是指某些著名的数据结构和算法，如队列、栈、堆、二分查找、动态规划等。
# 重点
-  复杂度分析
-  20个基础的数据结构与算法：
10个数据结构：数组、链表、栈、队列、散列表、⼆叉树、堆、跳表、图、Trie树；
10个算法：递归、排序、⼆分查找、搜索、哈希算法、贪⼼算法、分治算法、回溯算法、动态规划、字符串匹配算法。
# 复杂度分析
通过运行代码再进行监控和统计来评价算法的执行时间和内存占用的方法叫事后统计法，受测试环境和数据规模的影响较大，测试结果可能无法反映算法的性能
## 大O表示法
在不运行代码的情况下粗略的估计算法的执行效率
思路：将一行代码的执行时间假设为1个unit_time，计算算法的总执行时间。可以得出结论：代码的总执行时间 T(n) 和每行代码的执行次数 n 成正比，写成公式就是
`T(n) = O(f(n)) // f(n) `表示每行代码执行次数的总和

## 大O时间复杂度
实际上并不具体表示代码真正的执⾏时间，⽽是表示代码执⾏时间随数据规模增⻓的变化趋势，所以，也叫作渐进时间复杂度（asymptotic time complexity），简称时间复杂度。
### 分析方法
1. 只关注执行次数最多的一段代码
2. 加法法则：总复杂度等于量级最大的那段代码的复杂度

# 数据结构

## 数组

### 定义

**线性表**数据结构.用一组**连续的内存空间**存储一组**相同类型**的数据

- 线性表:数据排列成线一样的结构,每个数据最多有前后两个方向.比如:栈,链表,队列等

- 非线性表:数据之间不是简单的前后关系

- 连续的内存空间和型同类型的数据:有**随机访问的特性**,但也让很多操作变得低效,比如删除和插入

### 插入

有序:每个位置插入的时间复杂度是n

无序:只当作一个存储集合,可以插入到固定位置k,将k位置的元素放到最后,时间复杂度是1

### 删除

和插入类似

在一些特殊场景下,不一定追求连续性,将多次删除操作一起执行可以提高效率,比如只是记录数据已经被删除,在没有更多的空间时,一次性删除,也是JVM标记清除垃圾回收机制的核心思想

### 数组的访问越界

在C中是⼀种未决⾏为，并没有规定数组访问越界时编译器应该如何处理。因为，访问数组的本质就是访问⼀段连续内存，只要数组通过偏移计算得到的内存地址是可⽤的，那么程序就可能不会报任何错误。

很多计算机病毒也正是利⽤到了代码中的数组越界可以访问⾮法地址的漏洞，来攻击系统，所以写代码的时候⼀定要警惕数组越界。

Java本身就会做越界检查，会抛出java.lang.ArrayIndexOutOfBoundsException。

### 容器

针对数组类型，很多语⾔都提供了容器类，⽐如Java中的ArrayList、C++ STL中的vector。

ArrayList最⼤的优势就是可以将很多数组操作的细节封装起来。⽐如前⾯提到的数组插⼊、删除数据时需要搬移其他数据等。另外，它还有⼀个优势，就是⽀持动态扩容。每次存储空间不够的时候，它都会将空间⾃动扩容为1.5倍⼤⼩。这⾥需要注意⼀点，因为扩容操作涉及内存申请和数据搬移，是⽐较耗时的。所以，最好在创建ArrayList的时候事先指定数据⼤⼩。

1.Java ArrayList⽆法存储基本类型，⽐如int、long，需要封装为Integer、Long类，⽽Autoboxing、Unboxing则有⼀定的性能消耗，所以如果特别关注性能，或者希望使⽤基本类型，就可以选⽤数组。

2.如果数据⼤⼩事先已知，并且对数据的操作⾮常简单，⽤不到ArrayList提供的⼤部分⽅法，也可以直接使⽤数组。

3.当要表示多维数组时，⽤数组往往会更加直观。⽐如Object[][] array；⽽⽤容器的话则需要这样定义：ArrayList<ArrayList > array。

**总结**：如果是做⼀些⾮常底层的开发，⽐如开发⽹络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为⾸选。

> 为什么⼤多数编程语⾔中，数组要从0开始编号，⽽不是从1开始呢？

从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前⾯也讲到，如果⽤a来表示数组的⾸地址，a[0]就是偏移为0的位置，也就是⾸地址

计算a[k]的内存地址只需要⽤这个公式：a[k]_address = base_address + (k-1)\*type_size

C语⾔设计者⽤0开始计数数组下标，之后的Java、JavaScript等⾼级语⾔都效仿了C语⾔，为了减少学习成本

## 链表

### LRU缓存淘汰算法

当缓存被⽤满时就需要缓存淘汰策略来决定。常⻅的有三种：先进先出策略FIFO（First In，First Out）、最少使⽤策略LFU（Least Frequently Used）、最近最少使⽤策略LRU（Least Recently Used）。

从底层的存储结构来看：它通过“指针”将⼀组零散的内存块串联起来使⽤，其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下⼀个结点的地址。这个记录下个结点地址的指针叫作后继指针next。

### 单链表

头结点⽤来记录链表的基地址。有了它，我们就可以遍历得到整条链表。

尾结点的指针不是指向下⼀个结点，⽽是指向⼀个空地址NULL，表示这是链表上最后⼀个结点。

**插入和删除**：

因为链表的存储空间本身就不是连续的。所以，在链表中插⼊和删除⼀个数据是⾮常快速的。对应的时间复杂度是O(1)。

随机访问：因为链表中的数据并⾮连续存储的，需要根据指针⼀个结点⼀个结点地依次遍历，直到找到相应的结点。

### 循环链表

⼀种特殊的单链表。它跟单链表唯⼀的区别就在尾结点。循环链表的尾结点指针是指向链表的头结点。它像⼀个环⼀样⾸尾相连，所以叫作“循环”链表。

循环链表的优点是从链尾到链头⽐较⽅便。当要处理的数据具有环型结构特点时，就特别适合采⽤循环链表。⽐如著名的约瑟夫问题。

### 双向链表

它⽀持两个⽅向，每个结点不⽌有⼀个后继指针next指向后⾯的结点，还有⼀个前驱指针prev指向前⾯的结点。

双向链表要⽐单链表占⽤更多的内存空间。虽然两个指针⽐较浪费存储空间，但可以⽀持双向遍历，这样也带来了双向链表操作的灵活性。

**插入和删除**：

删除结点中“值等于某个给定值”的结点：从头结点开始⼀个⼀个依次遍历对⽐，直到找到值等于给定值的结点，然后再指针操作将其删除。对应的时间复杂度为O(n)。

删除给定指针指向的结点：我们已经找到了要删除的结点，删除某个结点q需要知道其前驱结点，对于双向列表来说是已知的，所以时间复杂度为O(1)*

Java中的LinkedHashMap这个容器的实现原理，其中就⽤到了双向链表这种数据结构。

### 双向循环链表

### 数组和链表

两种截然不同的内存组织⽅式。正是因为内存存储的区别，它们插⼊、删除、随机访问操作的时间复杂度正好相反。

数组简单易⽤，在实现上使⽤的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以**访问效率更⾼**。

链表在内存中并不是连续存储，所以**对CPU缓存不友好，没办法有效预读**。

数组的缺点是**⼤⼩固定**，⼀经声明就要占⽤整块连续内存空间。如果声明的数组过⼤，可能导致“内存不⾜（out of memory）”。如果声明的数组过⼩，则可能出现不够⽤的情况。这时只能再申请⼀个更⼤的内存空间，把原数组拷⻉进去，⾮常费时。

链表本身没有⼤⼩的限制，天然地⽀持**动态扩容**，这也是它与数组最⼤的区别。

### 书写技巧

#### 理解引用或指针

有些语⾔有“指针”的概念，⽐如C语⾔；有些语⾔没有指针，取⽽代之的是“引⽤”，⽐如Java、Python。不管是“指针”还是“引⽤”，实际上，它们的意思都是⼀样的，都是存储所指对象的内存地址。

**将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。**

p->next=q：p结点中的next指针存储了q结点的内存地址。
p->next=p->next->next：p结点的next指针存储了p结点的下下⼀个结点的内存地址。

#### 警惕指针丢失和内存泄漏

我们希望在结点a和相邻的结点b之间插⼊结点x，假设当前指针p指向结点a。如果我们将代码实现变成下⾯这个样⼦，就会发⽣指针丢失和内存泄露。

```c
p->next = x; // 将p的next指针指向x结点；
x->next = p->next; // 相当于将x赋值给x->next，⾃⼰指向⾃⼰，整个链表也就断成了两半，从结点b往后的所有结点都⽆法访问到了。
```

对于有些语⾔来说，⽐如C语⾔，内存管理是由程序员负责的，如果没有⼿动释放结点对应的内存空间，就会产⽣内存泄露。所以，我们**插⼊结点时，⼀定要注意操作的顺序**，要先将结点x的next指针指向结点b，再把结点a的next指针指向结点x，这样才不会丢失指针，导致内存泄漏。所以，对于刚刚的插⼊代码，我们只需要把第1⾏和第2⾏代码的顺序颠倒⼀下就可以了。

#### 利用哨兵简化实现难度

针对链表的**插⼊、删除**操作，需要对插⼊第⼀个结点和删除**最后⼀个结点的情况进⾏特殊处理**。这样代码实现起来就会很繁琐，不简洁，⽽且也容易因为考虑不全⽽出错。哨兵，解决的是国家之间的边界问题。同理，这⾥说的哨兵也是解决“边界问题”的，不直接参与业务逻辑。

在任何时候，不管链表是不是空，head指针都会⼀直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫**带头链表**。相反，没有哨兵结点的链表就叫作**不带头链表**。你可以发现，哨兵结点是**不存储数据**的。因为哨兵结点⼀直存在，所以插⼊第⼀个结点和插⼊其他结点，删除最后⼀个结点和删除其他结点，都可以统⼀为相同的代码实现逻辑了。

这种利⽤哨兵简化编程难度的技巧，在很多代码实现中都有⽤到，⽐如插⼊排序、归并排序、动态规划等。

#### 留意边界条件处理

- 如果链表为空时，代码是否能正常⼯作？
- 如果链表只包含⼀个结点时，代码是否能正常⼯作？
- 如果链表只包含两个结点时，代码是否能正常⼯作？
- 代码逻辑在处理头结点和尾结点的时候，是否能正常⼯作？

针对不同的场景，可能还有特定的边界条件，

#### 举例画图

#### 多写多练

5个常⻅的链表操作

- 单链表反转
- 链表中环的检测
- 两个有序的链表合并
- 删除链表倒数第n个结点
- 求链表的中间结点

## 栈

### 理解栈

后进者先出，先进者后出，这就是典型的“栈”结构。

数组或链表暴露了太多的操作接⼝，操作上的确灵活⾃由，但使⽤时就⽐较不可控，⾃然也就更容易出错。当某个数据集合只涉及在⼀端插⼊和删除数据，并且满⾜后进先出、先进后出的特性，我们就应该⾸选“栈”这种数据结构。

### 实现栈

栈既可以⽤数组来实现，也可以⽤链表来实现。⽤数组实现的栈，我们叫作顺序栈，⽤链表实现的栈，我们叫作链式栈。

不管是顺序栈还是链式栈，我们存储数据只需要⼀个⼤⼩为n的数组就够了。在⼊栈和出栈过程中，只需要⼀两个临时变量存储空间，所以空间复杂度是O(1)。(我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运⾏还需要额外的存储空间。)

不管是顺序栈还是链式栈，⼊栈、出栈只涉及栈顶个别数据的操作，所以时间复杂度都是O(1)。

#### 动态扩容的顺序栈

实现⼀个⽀持动态扩容的栈，我们只需要底层依赖⼀个⽀持动态扩容的数组就可以了。当栈满了之后，我们就申请⼀个更⼤的数组，将原来的数据搬移到新数组中。

以出栈的时间复杂度是O(1)，对于⼊栈操作来说，最好情况时间复杂度是O(1)，最坏情况时间复杂度是O(n)。均摊时间复杂度⼀般都等于最好情况时间复杂度。因为在⼤部分情况下，⼊栈操作的时间复杂度O都是O(1)，只有在个别时刻才会退化为O(n)，所以把耗时多的⼊栈操作的时间均摊到其他⼊栈操作上，平均情况下的耗时就接近O(1)。

### 栈的应用

#### 栈在函数调用中

操作系统给每个线程分配了⼀块独⽴的内存空间，这块内存被组织成“栈”这种结构,⽤来存储函数调⽤时的临时变量。每进⼊⼀个函数，就会将临时变量作为⼀个栈帧⼊栈，当被调⽤函数执⾏完成，返回之后，将这个函数对应的栈帧出栈。

#### 栈在表达式求值中

实际上，编译器就是通过两个栈来实现的。其中⼀个保存操作数的栈，另⼀个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压⼊操作数栈；当遇到运算符，就与运算符栈的栈顶元素进⾏⽐较。如果⽐运算符栈顶元素的优先级⾼，就将当前运算符压⼊栈；如果⽐运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取2个操作数，然后进⾏计算，再把计算完的结果压⼊操作数栈，继续⽐较。

#### 栈在括号匹配中

我们⽤栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压⼊栈中；当扫描到右括号时，从栈顶取出⼀个左括号。如果能够匹配，⽐如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为⾮法格式。当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为⾮法格式。

leedcode0020

#### 浏览器的前进后退

我们使⽤两个栈，X和Y，我们把⾸次浏览的⻚⾯依次压⼊栈X，当点击后退按钮时，再依次从栈X中出栈，并将出栈的数据依次放⼊栈Y。当我们点击前进按钮时，我们依次从栈Y中取出数据，放⼊栈X中。当栈X中没有数据时，那就说明没有⻚⾯可以继续后退浏览了。当栈Y中没有数据，那就说明没有⻚⾯可以点击前进按钮浏览了。

## 队列

### 理解队列

先进者先出，就是典型的“队列”。

栈只⽀持两个基本操作：⼊栈push()和出栈pop()。队列跟栈⾮常相似，⽀持的操作也很有限，最基本的操作也是两个：⼊队enqueue()，放⼀个数据到队列尾部；出队dequeue()，从队列头部取⼀个元素。所以，队列跟栈⼀样，也是⼀种操作受限的线性表数据结构。

### 实现队列

跟栈⼀样，队列可以⽤数组来实现，也可以⽤链表来实现。⽤数组实现的栈叫作顺序栈，⽤链表实现的栈叫作链式栈。同样，⽤数组实现的队列叫作顺序队列，⽤链表实现的队列叫作链式队列。

对于栈来说，我们只需要⼀个栈顶指针就可以了。但是队列需要两个指针：⼀个是head指针，指向队头；⼀个是tail指针，指向队尾。随着不停地进⾏⼊队、出队操作，head和tail都会持续往后移动。当tail移动到最右边，即使数组中还有空闲空间，也⽆法继续往队列中添加数据了。我们只需要在⼊队时，再集中触发⼀次数据的搬移操作。

### 循环队列

⽤数组来实现队列的时候，在tail==n时，会有数据搬移操作，这样⼊队操作性能就会受到影响。那有没有办法能够避免数据搬移呢？我们来看看循环队列的解决思路。

![循环队列-1](E:\Jennifer\other\notes\media\循环队列-1.png)

这个队列的⼤⼩为8，当前head=4，tail=7。当有⼀个新的元素a⼊队时，我们放⼊下标为7的位置。但这
个时候，我们并不把tail更新为8，⽽是将其在环中后移⼀位，到下标为0的位置。当再有⼀个元素b⼊队时，我们将b放⼊下标为0的位置，然后tail加1更新为1。

![循环队列-2](E:\Jennifer\other\notes\media\循环队列-2.png)

要想写出没有bug的循环队列的实现代码，最关键的是，确定好队空和队满的判定条件。队列为空的判断条件仍然是head == tail。但队列满的判断条件就稍微有点复杂了。

![循环队列-队满](E:\Jennifer\other\notes\media\循环队列-队满.png)

就像我图中画的队满的情况，tail=3，head=4，n=8，所以总结⼀下规律就是：(3+1)%8=4。多画⼏张队满的图，你就会发现，当队满时，(tail+1)%n=head。你有没有发现，当队列满时，图中的tail指向的位置实际上是没有存储数据的。所以，循环队列会浪费⼀个数组的存储空间。

### 阻塞队列和并发队列

队列这种数据结构很基础，平时的业务开发不⼤可能从零实现⼀个队列，甚⾄都不会直接⽤到。⽽⼀些具有特殊特性的队列应⽤却⽐较⼴泛，⽐如阻塞队列和并发队列。

**阻塞队列**其实就是在队列基础上增加了阻塞操作。简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插⼊数据的操作就会被阻塞，直到队列中有空闲位置后再插⼊数据，然后再返回。

上述的定义就是⼀个“⽣产者-消费者模型”！是的，我们可以使⽤阻塞队列，轻松实现⼀个“⽣产者-消费者
模型”！

这种基于阻塞队列实现的“⽣产者-消费者模型”，可以有效地协调⽣产和消费的速度。当“⽣产者”⽣产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，⽣产者就阻塞等待，直到“消费者”消费了数据，“⽣产者”才会被唤醒继续“⽣产”。⽽且不仅如此，基于阻塞队列，我们还可以通过协调“⽣产者”和“消费者”的个数，来提⾼数据的处理效率。⽐如前⾯的例⼦，我们可以多配置⼏个“消费者”，来应对⼀个“⽣产者”。

在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题，那如何实现⼀个线
程安全的队列呢？线程安全的队列我们叫作**并发队列**。最简单直接的实现⽅式是直接在enqueue()、dequeue()⽅法上加锁，但是锁粒度⼤并发度会⽐较低，同⼀时刻仅允许⼀个存或者取操作。实际上，基于数组的循环队列，利⽤CAS原⼦操作，可以实现⾮常⾼效的并发队列。这也是循环队列⽐链式队列应⽤更加⼴泛的原因。在实战篇讲Disruptor的时候，我会再详细讲并发队列的应⽤。

### 在线程池等有限资源池中的应用

CPU资源是有限的，任务的处理速度与线程个数并不是线性正相关。相反，过多的线程反⽽会导致CPU频繁切换，处理性能下降。所以，线程池的⼤⼩⼀般都是综合考虑要处理任务的特点和硬件环境，来事先设置的。当我们向固定⼤⼩的线程池中请求⼀个线程时，如果线程池中没有空闲资源了，这个时候线程池如何处理这个请求？是拒绝请求还是排队请求？各种处理策略⼜是怎么实现的呢？

我们⼀般有两种处理策略。第⼀种是⾮阻塞的处理⽅式，直接拒绝任务请求；另⼀种是阻塞的处理⽅式，将请求排队，等到有空闲线程时，取出排队的请求继续处理。所以队列这种数据结构很适合来存储排队请求。

我们前⾯说过，队列有基于链表和基于数组这两种实现⽅式。基于链表的实现⽅式，可以实现⼀个⽀持⽆限排队的⽆界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过⻓。所以，针对响应时间⽐较敏感的系统，基于链表实现的⽆限排队的线程池是不合适的。

⽽基于数组实现的有界队列（bounded queue），队列的⼤⼩有限，所以线程池中排队的请求超过队列⼤⼩时，接下来的请求就会被拒绝，这种⽅式对响应时间敏感的系统来说，就相对更加合理。不过，设置⼀个合理的队列⼤⼩，也是⾮常有讲究的。队列太⼤导致等待的请求太多，队列太⼩会导致⽆法充分利⽤系统资源、发挥最⼤性能。

除了前⾯讲到队列应⽤在线程池请求排队的场景之外，队列可以应⽤在任何有限资源池中，⽤于排队请求，⽐如数据库连接池等。实际上，对于⼤部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队。

## 递归

### 理解递归

所有的递归问题都可以⽤递推公式来表示。

#### 满足递归的三个条件

1. ⼀个问题的解可以分解为⼏个⼦问题的解
2. 这个问题与分解之后的⼦问题，除了数据规模不同，求解思路完全⼀样
3. 存在递归终⽌条件

### 编写递归代码

写递归代码最关键的是写出递推公式，找到终⽌条件，剩下将递推公式转化为代码就很简单了。

假如这⾥有n个台阶，每次你可以跨1个台阶或者2个台阶，请问⾛这n个台阶有多少种⾛法？
我们仔细想下，实际上，可以根据第⼀步的⾛法把所有⾛法分为两类，第⼀类是第⼀步⾛了1个台阶，另⼀类是第⼀步⾛了2个台阶。所以n个台阶的⾛法就等于先⾛1阶后，n-1个台阶的⾛法 加上先⾛2阶后，n-2个台阶的⾛法。⽤公式表示就是：

```js
f(n) = f(n-1)+f(n-2)
```

当有⼀个台阶时，我们不需要再继续递归，就只有⼀种⾛法。所以f(1)=1。这个递归终⽌条件⾜够吗？我们可以⽤n=2，n=3这样⽐较⼩的数试验⼀下。n=2时，f(2)=f(1)+f(0)。如果递归终⽌条件只有⼀个f(1)=1，那f(2)就⽆法求解了。所以除了f(1)=1这⼀个递归终⽌条件外，还要有f(0)=1，表示⾛0个台阶有⼀种⾛法，不过这样⼦就不符合正常的逻辑思维了。所以，我们可以把f(2)=2作为⼀种终⽌条件，表示⾛2个台阶，有两种⾛法，⼀步⾛完或者分两步来⾛。所以，递归终⽌条件就是f(1)=1，f(2)=2。这个时候，你可以再拿n=3，n=4来验证⼀下，这个终⽌条件是否⾜够并且正确。我们把递归终⽌条件和刚刚得到的递推公式放到⼀起就是这样的：

```js
f(1) = 1;
f(2) = 2;
f(n) = f(n-1)+f(n-2)
```

写递归代码的关键就是找到如何将⼤问题分解为⼩问题的规律，并且基于此写出递推公式，然后再推敲终⽌条件，最后将递推公式和终⽌条件翻译成代码。

对于递归代码，这种试图想清楚整个递和归过程的做法，实际上是进⼊了⼀个思维误区。很多时候，我们理解起来⽐较吃⼒，主要原因就是⾃⼰给⾃⼰制造了这种理解障碍。那正确的思维⽅式应该是怎样的呢？
如果⼀个问题A可以分解为若⼲⼦问题B、C、D，你可以假设⼦问题B、C、D已经解决，在此基础上思考如何解决问题A。⽽且，你只需要思考问题A与⼦问题B、C、D两层之间的关系即可，不需要⼀层⼀层往下思考⼦问题与⼦⼦问题，⼦⼦问题与⼦⼦⼦问题之间的关系。屏蔽掉递归细节，这样⼦理解起来就简单多了。因此，编写递归代码的关键是，只要遇到递归，我们就把它抽象成⼀个递推公式，不⽤想⼀层层的调⽤关系，不要试图⽤⼈脑去分解递归的每个步骤。

### 警惕堆栈溢出

我们可以通过在代码中限制递归调⽤的最⼤深度的⽅式来解决这个问题。递归调⽤超过⼀定深度（⽐如1000）之后，我们就不继续往下再递归了，直接返回报错。

但这种做法并不能完全解决问题，因为最⼤允许的递归深度跟当前线程剩余的栈空间⼤⼩有关，事先⽆法计算。如果实时计算，代码过于复杂，就会影响代码的可读性。所以，如果最⼤深度⽐较⼩，⽐如10、50，就可以⽤这种⽅法，否则这种⽅法并不是很实⽤。

### 警惕重复计算

想要计算f(5)，需要先计算f(4)和f(3)，⽽计算f(4)还需要计算f(3)，因此，f(3)就被计算了很多次，这就是重复计算问题。为了避免重复计算，我们可以通过⼀个数据结构（⽐如散列表）来保存已经求解过的f(k)。当递归调⽤到f(k)时，先看下是否已经求解过了。如果是，则直接从散列表中取值返回，不需要重复计算，这样就能避免刚讲的问题了。

### 其他问题

在时间效率上，递归代码⾥多了很多函数调⽤，当这些函数调⽤的数量较⼤时，就会积聚成⼀个可观的时间成本。在空间复杂度上，因为递归调⽤⼀次就会在内存栈中保存⼀次现场数据，所以在分析递归代码空间复杂度时，需要额外考虑这部分的开销，空间复杂度往往并不是O(1)，⽽是O(n)。

如果数据库⾥存在脏数据，我们还需要处理由此产⽣的⽆限递归问题。⽐如demo环境下数据库中，测试⼯程师为了⽅便测试，会⼈为地插⼊⼀些数据，就会出现脏数据。可以⽤限制递归深度来解决。不过，还有⼀个更⾼级的处理⽅法，就是⾃动检测A-B-C-A这种“环”的存在。

### 改为非递归

递归有利有弊，利是递归代码的表达⼒很强，写起来⾮常简洁；⽽弊就是空间复杂度⾼、有堆栈溢出的⻛险、存在重复计算、过多的函数调⽤会耗时较多等问题。所以，在开发过程中，我们要根据实际情况来选择是否需要⽤递归的⽅式来实现。

笼统地讲，所有的递归代码都可以改为迭代循环的⾮递归写法。因为递归本身就是借助栈来实现的，只不过我们使⽤的栈是系统或者虚拟机本身提供的，我们没有感知罢了。如果我们⾃⼰在内存堆上实现栈，⼿动模拟⼊栈、出栈过程，这样任何递归代码都可以改写成看上去不是递归代码的样⼦。

但是这种思路实际上是将递归改为了“⼿动”递归，本质并没有变，⽽且也并没有解决前⾯讲到的某些问题，徒增了实现的复杂度。

# 算法

## 排序

### 分析排序算法

#### 执行效率

1. 最好情况、最坏情况、平均情况时间复杂度
2. 时间复杂度的系数、常数 、低阶
3. ⽐较次数和交换（或移动）次数

#### 内存消耗

耗可以通过空间复杂度来衡量，原地排序（Sorted in place），就是特指空间复杂度是O(1)的排序算法。下面的三种排序算法，都是原地排序算法。

#### 稳定性

稳定性。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。比如，我们希望按照⾦额从⼩到⼤对订单数据排序。对于⾦额相同的订单，我们希望按照下单时间从早到晚有序。

### 冒泡排序

冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进⾏⽐较，看是否满⾜⼤⼩关系要求。如果不满⾜就让它俩互换。⼀次冒泡会让⾄少⼀个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序⼯作。的冒泡过程还可以优化。当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不⽤再继续执⾏后续的冒泡操作。我这⾥还有另外⼀个例⼦，这⾥⾯给6个元素排序，只需要4次冒泡操作就可以了。

1. 冒泡排序是**原地排序算法**，冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为O(1)，是⼀个原地排序算法。
2. 冒泡排序是**稳定的排序算法**吗，当有相邻的两个元素⼤⼩相等的时候，我们不做交换，相同⼤⼩的数据在排序前后不会改变顺序。
3. 冒泡排序的**时间复杂度**：最好情况下，我们只需要进⾏⼀次冒泡操作，所以最好情况时间复杂度是**O(n)**。⽽最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进⾏n次冒泡操作，所以最坏情况时间复杂度为**O(n2)**。

































