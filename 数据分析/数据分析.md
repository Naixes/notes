概述

数据分析的三个重要组成部分：

1. **数据采集**。它是我们的**原材料**，也是最“接地气”的部分，因为任何分析都要有数据源。
2. **数据挖掘**。它可以说是最“高大上”的部分，也是整个商业价值所在。之所以要进行数据分析，就是要找到其中的规律，来指导我们的业务。因此数据挖掘的核心是**挖掘数据的商业价值**，也就是我们所谈的**商业智能 BI**。
3. **数据可视化**。它可以说是数据领域中万金油的技能，可以让我们直观地了解到数据分析的结果。

img1

#### 数据采集

在数据采集部分中，你通常会和**数据源**打交道，然后使用工具进行**采集**。

- 数据源：
  - 获取常用的数据源

- 工具：
  - “八爪鱼”这个自动抓取的神器，可以帮你抓取 99% 的页面源。
  - 编写 Python 爬虫

img2

#### 数据挖掘

可以说是知识型的工程，相当于整个专栏中的“算法”部分。首先你要知道它的基本流程、十大算法、以及背后的数学基础。它会通过历史数据，告诉你未来会发生什么。
当然它也会告诉你这件事发生的置信度是怎样的，

一些概念：关联分析，Adaboost 算法

img3

#### 数据可视化

两种方法：

**使用 Python**：在 Python 对数据进行清洗、挖掘的过程中，我们可以使用Matplotlib、Seaborn 等第三方库进行呈现。
**第三方工具**：如果你已经生成了 csv 格式文件，想要采用所见即所得的方式进行呈现，可以采用微图、DataV、Data GIF Maker 等第三方工具，它们可以很方便地对数据进行处理，还可以帮你制作呈现的效果

### 数据挖掘

#### 基本流程

1. **商业理解**：我们的目的是更好地帮助业务，所以第一步我们要**从商业的角度理解项目需求**，在这个基础上，**再对数据挖掘的目标进行定义**。
2. **数据理解**：尝试**收集部分数据**，然后对数据进行**探索**，包括数据描述、数据质量验证等。这有助于你对收集的数据有个初步的认知。
3. **数据准备**：开始**收集数据**，并对数据进行**清洗**、数据**集成**等操作，**完成数据挖掘前的准备工作**。
4. **模型建立**：选择和应用各种数据挖掘模型，并进行优化，以便得到更好的分类结果。
5. **模型评估**：对模型进行评价，并检查构建模型的每个步骤，**确认模型是否实现了预定的商业目标**。
6. **上线发布**：模型的作用是从数据中找到金矿，也就是我们所说的“知识”，**获得的知识需要转化成用户可以使用的方式**，呈现的形式可以是一份报告，也可以是实现一个比较复杂的、可重复的数据挖掘过程。数据挖掘结果如果是日常运营的一部分，那么后续的监控和维护就会变得重要。

> 注：理解项目的商业需求----确立数据挖掘目标----探索部分数据形成初步认知----数据的收集，清洗，集成，完成准备工作----应用和优化各种数据挖掘模型，得到更好的分类----评估模型是否实现了商业目标----发布，将知识转化为用户可以使用的方式

#### 十大算法

四类

##### 分类算法

**C4.5**

C4.5 算法是得票最高的算法，可以说是十大算法之首。C4.5 是**决策树的算法**，它创造性地在**决策树构造过程中就进行了剪枝**，并且**可以处理连续的属性**，也能**对不完整的数据进行处理**。它可以说是决策树分类中，具有里程碑式意义的算法。

**朴素贝叶斯（Naive Bayes）**

朴素贝叶斯模型是**基于概率论**的原理，它的思想是这样的：对于给出的未知物体想要进行分类，就需要**求解在这个未知物体出现的条件下各个类别出现的概率**，哪个最大，就认为这个未知物体属于哪个分类。

**SVM**

SVM 的中文叫**支持向量机**，英文是 Support Vector Machine，简称 SVM。SVM 在训练中建立了一个**超平面的分类模型**。

**KNN**

KNN 也叫 **K 最近邻算法**，英文是 K-Nearest Neighbor。所谓 K 近邻，就是**每个样本都可以用它最接近的 K 个邻居来代表**。如果一个样本，它的 K 个最接近的邻居都属于分类A，那么这个样本也属于分类 A。

**Adaboost**

Adaboost 在训练中建立了一个**联合的分类模型**。boost 在英文中代表提升的意思，所以Adaboost 是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类器，所以 Adaboost 也是一个常用的分类算法。

**CART**

CART 代表**分类和回归树**，英文是 Classification and Regression Trees。像英文一样，它构建了两棵树：一颗是分类树，另一个是回归树。和 C4.5 一样，它是一个**决策树学习方法**。

##### 聚类算法

**K-Means**

K-Means 算法是一个聚类算法。你可以这么理解，最终我想把物体划分成 K 类。假设每个类别里面，都有个“中心点”，即意见领袖，它是这个类别的核心。现在我有一个新点要归类，这时候就只要**计算这个新点与 K 个中心点**的距离，距离哪个中心点近，就变成了哪个类别。

**EM**

EM 算法也叫**最大期望算法**，是**求参数的最大似然估计**的一种方法。原理是这样的：假设我们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到 B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。
EM 算法经常用于**聚类和机器学习领域**中。

##### 关联分析

**Apriori**

Apriori 是一种**挖掘关联规则（association rules）的算法**，它通过**挖掘频繁项集**（frequent item sets）来揭示物品之间的关联关系，被广泛应用到**商业挖掘和网络安全等领域**中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系。

##### 连接分析

**PageRank**

PageRank 起源于**论文影响力**的计算方式，如果一篇文论被引入的次数越多，就代表这篇论文的影响力越强。同样 PageRank 被 Google 创造性地应用到了**网页权重**的计算中：当一个页面链出的页面越多，说明这个页面的“参考文献”越多，当这个页面被链入的频率越高，说明这个页面被引用的次数越高。基于这个原理，我们可以得到网站的权重划分。

这 10 个经典算法在整个数据挖掘领域中的得票最高的，后面的一些其他算法也基本上都是在这个基础上进行改进和创新。

#### 背后的数学原理

**概率论与数理统计**：比如条件概率、独立性的概念，以及随机变量、多维随机变量的概念

**线性代数**：向量和矩阵，比如我们经常会把对象抽象为矩阵的表示，一幅图像就可以抽象出来是一个矩阵，我们也经常计算特征值和特征向量，用特征向量来近似代表物体的特征。这个是大数据降维的基本思路。

基于矩阵的各种运算，以及基于矩阵的理论成熟，可以帮我们解决很多实际问题，比如PCA 方法、SVD 方法，以及 MF、NMF 方法等在数据挖掘中都有广泛的应用。

**图论**：社交网络的兴起，让图论的应用也越来越广。人与人的关系，可以用图论上的两个节点来进行连接，节点的度可以理解为一个人的朋友数。我们都听说过人脉的六度理论，在Facebook 上被证明平均一个人与另一个人的连接，只需要 3.57 个人。当然图论对于网络结构的分析非常有效，同时图论也在关系挖掘和图像分割中有重要的作用。

img4

### Python基础语法

#### 输入输出

```python
# raw_input 是 Python2.7 的输入函数，在 python3.x 里可以直接使用 input
name = raw_input("What's your name?")
sum = 100+100
print ('hello,%s' %name)
print ('sum = %d' %sum)
```

#### 判断

```python
# 没有括号，注意冒号，使用缩进表示层级
if score>= 90:
	print 'Excellent'
else:
	if score < 60:
		print 'Fail'
    else:
    	print 'Good Job'
```

#### 循环

```python
sum = 0
# for in 表示循环，可以控制循环次数
# range 函数，它在 for 循环中比较常用。range(11) 代表从 0 到 10，相当于 range(0,11)，range 还可以增加步长，比如 range(1,11,2) 代表的是[1,3,5,7,9]。

for number in range(11):
     sum = sum + number
print sum

# while：条件循环
sum = 0
number = 1
while number < 11:
     sum = sum + number
     number = number + 1
print sum
```

#### 数据类型

列表、元组、字典、集合

##### 列表：[]

```python
# 相当于数组，具有增删改查的功能

lists = ['a','b','c']
# append()方法
lists.append('d')
print lists
# len()方法
print len(lists)
# insert()方法
lists.insert(0,'mm')
# pop()方法
lists.pop()
print lists
```

##### 元组 (tuple)

```python
#  tuple 一旦初始化就不能修改。可以像访问数组一样进行访问，比如 tuples[0]，但不能赋值。

tuples = ('tupleA','tupleB')
print tuples[0]
```

##### 字典 {dictionary}

```python
# -*- coding: utf-8 -*

# 字典其实就是{key, value}，多次对同一个 key 放入 value，后面的值会把前面的值冲掉，同样字典也有增删改查。

# 定义一个 dictionary
score = {'guanyu':95,'zhangfei':96}
# 添加一个元素
score['zhaoyun'] = 98
print score
# 删除一个元素：pop()
score.pop('zhangfei')
# 查看 key 是否存在：in
print 'guanyu' in score
# 查看一个 key 对应的值：get()
print score.get('guanyu')
# 如果查询的值不存在，我们也可以给一个默认
print score.get('yase',99)
```

##### 集合：set

```python
# 集合 set 和字典 dictory 类似，不过它只是 key 的集合，不存储 value。同样可以增删查。

s = set(['a', 'b', 'c'])
# add()
s.add('d')
# remove()
s.remove('b')
print s
# 查询看某个元素是否在这个集合里：in
print 'c' in s
```

#### 注释

```python
# 如果注释中有中文，一般会在代码前添加 # -- coding: utf-8-
# -*- coding: utf-8 -*
# 如果是多行注释，使用三个单引号，或者三个双引号
'''
这是多行注释，用三个单引号
这是多行注释，用三个单引号
这是多行注释，用三个单引号
'''
```

#### 引用模块 / 包：import

```python
# import 的本质是路径搜索。import 引用可以是模块module，或者包 package。

# 针对 module，实际上是引用一个.py 文件。
# 导入一个模块
import model_name
# 导入多个模块
import module_name1,module_name2

# 针对 package，可以采用 from … import…的方式，这里实际上是从一个目录中引用模块，这时目录结构中必须带有一个__init__.py 文件。
# 导入包中指定模块
from package_name import moudule_name
```

#### 函数：def

```python
def addone(score):
     return score + 1
print addone(99)
```

### NumPy

它不仅是 Python 中使用最多的第三方库，而且还是 SciPy、Pandas 等数据科学的基础库。它所提供的数据结构比 Python 自身的“更高级、更高效”，可以这么说，NumPy 所提供的数据结构是 Python 数据分析的基础。

#### 用数组代替列表

标准的 Python 中，用列表 list 保存数组的数值。由于列表中的元素可以是任意的对象，所以列表中 **list 保存的是对象的指针**。虽然在 Python 编程中隐去了指针的概念，但是数组有指针，Python 的列表 list 其实就是数组。这样如果我要保存一个简单的数组 [0,1,2]，就需要有 3 个指针和 3 个整数的对象，浪费了内存和计算时间。

列表 list 的元素在系统内存中是**分散存储**的，而 NumPy 数组存储在一个均匀连续的内存块中。这样数组计算遍历所有的元素，不像列表 list 还需要对内存地址进行查找，从而节省了计算资源。

在内存访问模式中，缓存会直接把字节块从 RAM 加载到 CPU 寄存器中。因为数据连续的存储在内存中，NumPy 直接利用现代 **CPU 的矢量化指令计算**，加载寄存器中的多个连续浮点数。另外 NumPy 中的**矩阵计算可以采用多线程的方式**，充分利用多核 CPU 计算资源，大大提升了计算效率。

除了使用 NumPy 外，你还需要一些技巧来提升内存和提高计算资源的利用率。一个重要的规则就是：**避免采用隐式拷贝，而是采用就地操作**的方式。如果我想让一个数值 x 是原来的两倍，可以直接写成 x*=2，而不要写成 y=x*2。

#### ndarray 对象

ndarray 实际上是**多维数组**的含义。在 NumPy 数组中，维数称为**秩（rank）**，一维数组的秩为 1，二维数组的秩为 2，以此类推。在 NumPy 中，每一个线性的数组称为一个**轴（axes）**，其实秩就是描述轴的数量。

##### 创建数组

```python
import numpy as np
a = np.array([1, 2, 3])
b = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
# 赋值修改数组的值
b[1,1]=10
# shape 属性获得数组的大小
print a.shape
print b.shape
# dtype获得元素的属性
print a.dtype
print b
# (3L,)
# (3L, 3L)
# int32
# [[ 1 2 3]
#  [ 4 10 6]
#  [ 7 8 9]]

```

> shape理解：假设其shape是（2，3，4）。这个shape理解：2个二维数组，每个二维数组有3个一维数组，每个一维数组有4个元素。

##### 结构数组

在 C 语言里，可以定义结构数组，也就是通过 struct 定义结构类型，在 numpy 中：

```python
import numpy as np
# 用 dtype 定义的结构类型
persontype = np.dtype({
    'names':['name', 'age', 'chinese', 'math', 'english'],
    'formats':['S32','i', 'i', 'i', 'f']})
# 用 array 中指定了结构数组的类型 dtype=persontype
peoples = np.array([("ZhangFei",32,75,100, 90),("GuanYu",24,85,96,88.5),
    ("ZhaoYun",28,85,92,96.5),("HuangZhong",29,65,85,100)],
    dtype=persontype)
# 获取每个人的年龄
ages = peoples[:]['age']
chineses = peoples[:]['chinese']
maths = peoples[:]['math']
englishs = peoples[:]['english']
# np.mean()计算平均值
print np.mean(ages)
print np.mean(chineses)
print np.mean(maths)
print np.mean(englishs)
# 28.25
# 77.5
# 93.25
# 93.75


```

#### ufunc 运算

ufunc 是 universal function 的缩写，它**能对数组中每个元素进行函数操作**。NumPy 中很多 ufunc 函数计算速度非常快，因为都是采用 **C 语言实现**的。

##### 连续数组的创建

**arange()** 类似内置函数 range()，通过指定**初始值、终值、步长**来创建等差数列的一维数组，默认是**不包括终值**的。

**linspace()** 是 linear space 的缩写，代表线性等分向量的含义。通过指定**初始值、终值、元素个数**来创建等差数列的一维数组，默认是**包括终值**的。

```python
# np.arange 和 np.linspace 起到的作用是一样的，都是创建等差数组。这两个数组的结果x1,x2 都是 [1 3 5 7 9]。结果相同，但是你能看出来创建的方式是不同的。

# arange() 通过指定初始值、终值、步长来创建等差数列的一维数组，默认是不包括终值的。
x1 = np.arange(1,11,2)

# linspace() 通过指定初始值、终值、元素个数来创建等差数列的一维数组，默认是包括终值的。
x2 = np.linspace(1,9,5)
```

##### 算数运算

通过 NumPy 可以自由地创建等差数组，同时也可以进行加、减、乘、除、求 n 次方和取余数。

```python
x1 = np.arange(1,11,2) # 1 3 5 7 9
x2 = np.linspace(1,9,5) # 1 3 5 7 9
print np.add(x1, x2)
print np.subtract(x1, x2)
print np.multiply(x1, x2)
print np.divide(x1, x2)
print np.power(x1, x2)
# 在取余函数里，既可以用 np.remainder(x1, x2)，也可以用 np.mod(x1, x2)，结果是一样的。
print np.remainder(x1, x2)

# [ 2. 6. 10. 14. 18.]
# [0. 0. 0. 0. 0.]
# [ 1. 9. 25. 49. 81.]
# [1. 1. 1. 1. 1.]
# [1.00000000e+00 2.70000000e+01 3.12500000e+03 8.23543000e+05
#  3.87420489e+08]
# [0. 0. 0. 0. 0.]

```

##### 统计函数

如果你想要对一堆数据有更清晰的认识，就需要对这些数据进行描述性的统计分析，比如了解这些数据中的最大值、最小值、平均值，是否符合正态分布，方差、标准差多少等等。

**计数组 / 矩阵中的最大值函数 amax()，最小值函数 amin()**

```python
import numpy as np
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
# amin() 用于计算数组中的元素沿指定轴的最小值。
print np.amin(a)
# amin(a,0) 是延着 axis=0 轴的最小值，是把元素看成了 [1,4,7], [2,5,8], [3,6,9] 三个元素
print np.amin(a,0) # 在shape中表示的是一维数组，求一维数组对应元素的最小值
# amin(a,1) 是延着 axis=1 轴的最小值，是把元素看成了 [1,2,3], [4,5,6], [7,8,9] 三个元素
print np.amin(a,1) # 在shape中表示的是元素，求每一个一维数组元素中的最小值
print np.amax(a)
print np.amax(a,0)
print np.amax(a,1)

# 1
# [1 2 3]
# [1 4 7]
# 9
# [7 8 9]
# [3 6 9]
```

> ???
>
> axis理解：如果a是三维数组。假设其shape是（2，3，4）。axis等于0时，在shape中表示的是二维数组。那么np.amin(a,0)方法就是求二维数组对应元素的最小值，最终的结果的shape 正好是3个一维数组，一个一维数组是4个元素。
>
> 同理axis 等于1时，在shape中表示的是1维数组，那么np.amin(a,1)方法就是求一维数组对应的元素的最小值。由于每个一维数组的元素是4个，有个二维数组，所以最终的shape是（2，4）。
>
> 同理axis等于2时，在shape中表示的一维数组中的元素，意思是求每一个一维数组元素中的最小值，而三维数组一个有2个二维数组，每个二维数组有3个一维数组，所以最终过得shape就是（2，3）。

**统计最大值与最小值之差 ptp()**

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.ptp(a)
print np.ptp(a,0)
print np.ptp(a,1)

# 8
# [6 6 6]
# [2 2 2]
```

**统计数组的百分位数 percentile()**

```python
# percentile() 代表着第 p 个百分位数，这里 p 的取值范围是 0-100，如果 p=0，那么就是求最小值，如果 p=50 就是求平均值，如果 p=100 就是求最大值。同样你也可以求得在 axis=0 和 axis=1 两个轴上的 p% 的百分位数。
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
print np.percentile(a, 50)
print np.percentile(a, 50, axis=0)
print np.percentile(a, 50, axis=1)

# 5.0
# [4. 5. 6.]
# [2. 5. 8.]
```

**统计数组中的中位数 median()、平均数 mean()**

```python
a = np.array([[1,2,3], [4,5,6], [7,8,9]])
# 求中位数
print np.median(a)
print np.median(a, axis=0)
print np.median(a, axis=1)
# 求平均数
print np.mean(a)
print np.mean(a, axis=0)
print np.mean(a, axis=1)

# 5.0
# [4. 5. 6.]
# [2. 5. 8.]
# 5.0
# [4. 5. 6.]
# [2. 5. 8.]
```

**统计数组中的加权平均值 average()**

```python
a = np.array([1,2,3,4])
wts = np.array([1,2,3,4])
print np.average(a)
print np.average(a,weights=wts)

# 2.5
# 3.0
```

**统计数组中的标准差 std()、方差 var()**

方差的计算是指每个数值与平均值之差的平方求和的平均值上，代表的是一组数据离平均值的分散程度

标准差是方差的算术平方根

```python
a = np.array([1,2,3,4])
print np.std(a)
print np.var(a)

# 1.118033988749895
# 1.25
```

#### NumPy 排序

 sort 函数，sort(a, axis=-1, kind=‘quicksort’, order=None)

默认情况下使用的是快速排序；在 kind 里，可以指定 quicksort、mergesort、heapsort 分别表示快速排序、合并排序、堆排序。同样 axis 默认是 -1，即沿着数组的最后一个轴进行排序，也可以取不同的 axis 轴，或者 axis=None 代表采用扁平化的方式作为一个向量进行排序。另外 order 字段，对于结构化的数组可以指定按照某个字段进行排序。

```python
a = np.array([[4,3,2],[2,4,1]])
print np.sort(a)
print np.sort(a, axis=None)
print np.sort(a, axis=0)
print np.sort(a, axis=1)

# [[2 3 4]
# [1 2 4]]
# [1 2 2 3 4 4]
# [[2 3 1]
# [4 4 2]]
# [[2 3 4]
# [1 2 4]]
```

#### 总结

在 NumPy 学习中，重点就是对数组的使用，因为这是 NumPy 和标准Python 最大的区别。在 NumPy 中重新对数组进行了定义，同时提供了算术和统计运算，你也可以使用 NumPy 自带的排序功能，一句话就搞定各种排序算法。

### Pandas

Pandas 的使用频率是很高的，一方面是因为 Pandas 提供的**基础数据结构 DataFrame 与 json 的契合度很高**，转换起来就很方便。另一方面，如果我们日常的数据清理工作不是很复杂的话，你通常用**几句 Pandas 代码就可以对数据进行规整**。

Pandas 可以说是基于 NumPy 构建的含有更高级数据结构和分析能力的工具包。

#### 数据结构

**Series** 和 **DataFrame** 这两个核心数据结构，他们分别代表着一维的序列和二维的表结构。基于这两种数据结构，Pandas 可以对数据进行导入、清洗、处理、统计和输出。

##### Series

Series 是个**定长的字典序列**。说是定长是因为在存储的时候，**相当于两个 ndarray**，这也是和字典结构最大的不同。因为在字典的结构里，元素的个数是不固定的。

Series有两个基本属性：**index** 和 **values**。在 Series 结构中，index 默认是 0,1,2,……递增的整数序列，当然我们也可以自己来指定索引，比如 index=[‘a’, ‘b’, ‘c’,‘d’]。

```python
import pandas as pd
from pandas import Series, DataFrame
x1 = Series([1,2,3,4])
x2 = Series(data=[1,2,3,4], index=['a', 'b', 'c', 'd'])
print x1
print x2

# 0 1
# 1 2
# 2 3
# 3 4
# dtype: int64
# a 1
# b 2
# c 3
# d 4
# dtype: int64

# 也可以采用字典的方式
d = {'a':1, 'b':2, 'c':3, 'd':4}
x3 = Series(d)
print x3

# a 1
# b 2
# c 3
# d 4
# dtype: int64
```

##### DataFrame

DataFrame 类型数据结构类似数据库表，它**包括了行索引和列索引**，我们可以将 DataFrame 看成是由相同索引的 Series 组成的字典类型。

```python
import pandas as pd
from pandas import Series, DataFrame
data = {'Chinese': [66, 95, 93, 90,80],'English': [65, 85, 92, 88, 90],'Math': [30, 98, 96, 77, 90]}
df1= DataFrame(data)
df2 = DataFrame(data, index=['ZhangFei', 'GuanYu', 'ZhaoYun', 'HuangZhong', 'DianWei'],
print df1
print df2
                
#  		   English Math Chinese
# ZhangFei    65     30    66
# GuanYu      85     98    95
# ZhaoYun     92     96    93
# HuangZhong  88     77    90
# DianWei     90     90    80
```

#### 数据导入和导出

Pandas 允许直接从 **xlsx，csv** 等文件中导入数据，也可以输出到 xlsx, csv 等文件，非常方便。

```python
import pandas as pd
from pandas import Series, DataFrame
score = DataFrame(pd.read_excel('data.xlsx'))
score.to_excel('data1.xlsx')
print score
```

在运行的过程可能会存在缺少 xlrd 和 openpyxl 包的情况，到时候如果缺少了，可以在命令行模式下使用“pip install”命令来进行安装。

#### 数据清洗

Pandas 也为我们提供了数据清洗的工具

1. **删除 DataFrame 中的不必要的列或行**

`df2 = df2.drop(columns=['Chinese'])`

`df2 = df2.drop(index=['ZhangFei'])`

2. **重命名列名 columns，让列表名更容易识别**

可以直接使用`rename(columns=new_names, inplace=True)`函数，比如我把列名 Chinese 改成
YuWen，English 改成 YingYu。

` df2.rename(columns={'Chinese': 'YuWen', 'English': 'Yingyu'}, inplace = True)`

3. **去重复的值**

`df = df.drop_duplicates() # 去除重复行`

4. **格式问题**

   **更改数据格式**

   可以使用` astype `函数来规范数据格式，比如我们把 Chinese 字段的值改成 str 类型，或者 int64 可以这么写

   ```python
   df2['Chinese'].astype('str')
   df2['Chinese'].astype(np.int64)
   ```

   **数据间的空格**

   ```python
   # 删除左右两边空格
   df2['Chinese']=df2['Chinese'].map(str.strip)
   # 删除左边空格
   df2['Chinese']=df2['Chinese'].map(str.lstrip)
   # 删除右边空格
   df2['Chinese']=df2['Chinese'].map(str.rstrip)
   
   # 删除特殊符号
   df2['Chinese']=df2['Chinese'].str.strip('$')
   ```

   **大小写转换**

   ```python
   # 全部大写
   df2.columns = df2.columns.str.upper()
   # 全部小写
   df2.columns = df2.columns.str.lower()
   # 首字母大写
   df2.columns = df2.columns.str.title()
   ```

   **查找空值**

   数据量大的情况下，有些字段存在空值 NaN 的可能，这时就需要使用 Pandas 中的 isnull函数进行查找。

   我们想看下哪个地方存在空值 NaN，可以针对数据表 df 进行 `df.isnull()`

   如果我想知道哪列存在空值，可以使用` df.isnull().any()`

**使用 apply 函数对数据进行清洗**

apply 函数是 Pandas 中自由度非常高的函数，使用频率也非常高。

```python
# 大写转换
df['name'] = df['name'].apply(str.upper)

# 在 apply 中使用自定义函数。比如定义 double_df 函数是将原来的数值 *2 进行返回。然后对 df1 中的“语文”列的数值进行 *2 处理
def double_df(x):
	return 2*x
df1[u'语文'] = df1[u'语文'].apply(double_df)
# 也可以定义更复杂的函数，比如对于 DataFrame，我们新增两列，其中’new1’列是“语文”和“英语”成绩之和的 m 倍，'new2’列是“语文”和“英语”成绩之和的 n倍
# 其中 axis=1 代表按照列为轴进行操作，axis=0 代表按照行为轴进行操作，args 是传递的两个参数，即 n=2, m=3，在 plus 函数中使用到了 n 和 m，从而生成新的 df。
def plus(df,n,m):
    df['new1'] = (df[u'语文']+df[u'英语']) * m
    df['new2'] = (df[u'语文']+df[u'英语']) * n
    return df
df1 = df1.apply(plus,axis=1,args=(2,3,))
```

#### 数据统计

Pandas 和 NumPy 一样，都有常用的统计函数，如果遇到空值 NaN，会自动排除。

常用的统计函数：

![pandas的常用统计函数](E:\Jennifer\other\notes\数据分析\media\pandas的常用统计函数.png)

表格中有一个 describe() 函数，它是个统计大礼包，可以快速让我们对数据有个全面的了解。下面我直接使用 df1.descirbe() 输出结果为：

```python
df1 = DataFrame({'name':['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1':range(5)})
print df1.describe()
```

![describe函数输出结果](E:\Jennifer\other\notes\数据分析\media\describe函数输出结果.png)

#### 数据表合并

。。。。。。

### 一些基本概念

**Apriori 算法**：该算法是由美国学者 Agrawal 在 1994 年提出的。他通过分析购物篮中的商品集合，找出商品之间的关联关系。利用这种隐性关联关系，商家就可以强化这类购买行为，从而提升销售额。

#### 商业智能 BI

Business Intelligence

百货商店利用数据预测用户购物行为属于商业智能，他们积累的顾客的消费行为习惯会存储在**数据仓库**中，通过对个体进行消费行为分析**总结出来的规律属于数据挖掘**。

相比于数据仓库、数据挖掘，它是一个更大的概念。商业智能可以说是**基于数据仓库，经过了数据挖掘后，得到了商业价值的过程**。所以说数据仓库是个金矿，数据挖掘是炼金术，而商业报告则是黄金。

#### 数据仓库 DW

Data Warehouse

数据仓库可以说是数据库的升级概念。从逻辑上理解，和数据库没有什么区别，都是通过数据库技术来存储数据的。不过从数量上来讲，数据仓库的**量更庞大**，**适用于数据挖掘和数据分析**。数据库可以理解是一项技术。

数据仓库将原有的多个数据来源中的数据进行**汇总、整理**而得。数据进入数据仓库前，**必须消除数据中的不一致性**，方便后续进行数据分析和挖掘。

#### 数据挖掘 DM

Data Mining

数据挖掘的核心包括**分类、聚类、预测、关联分析等任务**，通过这些炼金术，我们可以从数据仓库中得到宝藏，比如**商业报告**。

**三者之间的关系**

很多时候，企业老板总是以结果为导向，他们认为商业报告才是他们想要的，但是这也是需要经过地基 DW、搬运工 ETL、科学家 DM 等共同的努力才得到的。

#### 元数据 VS 数据元

**元数据（MetaData）**：描述其它数据的数据，也称为“中介数据”。

**数据元（Data Element）**：就是最小数据单元。

在生活中，只要有一类事物，就可以定义一套元数据。举个例子，比如一本图书的信息包括了书名、作者、出版社、ISBN、出版时间、页数和定价等多个属性的信息，我们就可以把这些属性定义成一套图书的元数据。

在图书这个元数据中，书名、作者、出版社就是数据元。你可以理解是最小的数据单元。元数据最大的好处是**使信息的描述和分类实现了结构化**，让机器处理起来很方便。

元数据可以很方便地应用于数据仓库。比如数据仓库中有数据和数据之间的各种复杂关系，为了描述这些关系，元数据**可以对数据仓库的数据进行定义，刻画数据的抽取和转换规则，存储与数据仓库主题有关的各种信息**。而且**整个数据仓库的运行都是基于元数据**的，比如抽取调度数据、获取历史数据等。

通过元数据，可以很方便地帮助我们管理数据仓库。

#### 数据挖掘的流程

数据挖掘的一个英文解释叫 Knowledge Discovery in Database，简称KDD，也就是数据库中的知识发现。在数据挖掘中，有几个非常重要的任务，就是**分类、聚类、预测和关联分析**。

- **分类**

就是通过**训练集**得到一个**分类模型**，然后用这个模型可以对其他数据进行分类。

一般来说数据可以划分为**训练集**和**测试集**。训练集是用来给机器做训练的，通常是人们整理好训练数据，以及这些数据对应的分类标识。通过训练，机器就产生了自我分类的模型，然后机器就可以拿着这个分类模型，对测试集中的数据进行分类预测。同样如果测试集中，人们已经给出了测试结果，我们就可以用测试结果来做验证，从而了解分类器在测试环境下的表现。

- **聚类**

聚类就是**将数据自动聚类成几个类别**，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做**数据划分**。

- **预测**

通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险。

- **关联分析**

发现数据中的关联规则，它被广泛应用在**购物篮分析，或事务数据分析**中。

**流程**

![数据挖掘流程](E:\Jennifer\other\notes\数据分析\media\数据挖掘流程.png)

**数据预处理**中，我们会对数据进行几个处理步骤：数据清洗，数据集成，以及数据变换。

1. 数据清洗
主要是为了去除重复数据，去噪声（即干扰数据）以及填充缺失值。
2. 数据集成
是将多个数据源中的数据存放在一个统一的数据存储中。
3. 数据变换
就是将数据转换成适合数据挖掘的形式。比如，通过归一化将属性数据按照比例缩放，这样就可以将数值落入一个特定的区间内，比如 0~1 之间。

**数据后处理**是将模型预测的结果进一步处理后，再导出。比如在二分类问题中，一般能得到的是 0~1 之间的概率值，此时把数据以 0.5 为界限进行四舍五入就可以实现后处理。

#### 白话数据概念

比如你认识了两个漂亮的女孩。

**商业智能**会告诉你要追哪个？成功概率有多大？

**数据仓库**会说，我这里存储了这两个女孩的相关信息，你要吗？其中每个女孩的数据都有单独的文件夹，里面有她们各自的姓名、生日、喜好和联系方式等，这些具体的信息就是**数据元**，加起来叫作**元数据**。

**数据挖掘**会帮助你确定追哪个女孩，并且整理好数据仓库，这里就可以使用到各种算法，帮你做决策了。你可能会用到**分类算法**。御姐、萝莉、女王，她到底属于哪个分类？如果认识的女孩太多了，多到你已经数不过来了，比如说 5 万人！你就可以使用**聚类算法**了，它帮你把这些女孩分成多个群组，比如 5 个组。然后再对每个群组的特性进行了解，进行决策。这样就把 5 万人的决策，转化成了 5 个组的决策。成功实现降维，大大提升了效率。如果你想知道这个女孩的闺蜜是谁，那么**关联分析算法**可以告诉你。如果你的数据来源比较多，比如有很多朋友给你介绍女朋友，很多人都推荐了同一个，你就需要去重，这叫**数据清洗**；为了方便记忆，你把不同朋友推荐的女孩信息合成一个，这叫**数据集成**；有些数据渠道统计的体重的单位是公斤，有些是斤，你就需要将它们转换成同一个单位，这叫**数据变换**。最后你可以进行**数据可视化**了，它会直观地把你想要的结果呈现出来。

### 用户画像

#### 用户画像的准则

首先就是将自己企业的用户画像做个白描，告诉他这些用户**“都是谁”“从哪来”“要去哪”**。

你可以这么和老板说：“老板啊，用户画像建模是个系统的工程，我们要解决三个问题。

第一呢，就是用户从哪里来，这里我们需要**统一标识用户 ID**，方便我们对用户后续行为进行跟踪。我们要了解这些羊肉串的用户从哪里来，他们是为了聚餐，还是自己吃宵夜，这些场景我们都要做统计分析。

第二呢，这些用户是谁？我们需要对这些用户进行**标签化**，方便我们对用户行为进行理解。

第三呢，就是用户要到哪里去？我们要将这些用户画像与我们的**业务相关联**，提升我们的转化率，或者降低我们的流失率。”

![用户画像建模](E:\Jennifer\other\notes\数据分析\media\用户画像建模.png)

##### 设计唯一标识

用户唯一标识是整个用户画像的**核心**。我们以一个 App 为例，它把从用户开始使用APP 到下单到售后整个所有的**用户行为进行串联**，这样就可以更好地去**跟踪和分析一个用户的特征**。设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID 等。

##### 给用户打标签

要既方便记忆，又能保证用户画像的全面性，这里总结了八个字，叫“**用户消费行为分析**”。我们可以从这 4 个维度来进行标签划分。

1. **用户标签**：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性。
2. **消费标签**：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。

3. **行为标签**：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。
4. **内容分析**：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等。

可以说，用户画像是现实世界中的用户的数学建模，我们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题。

##### 为企业带来业务价值

我们可以从**用户生命周期**的三个阶段来划分业务价值，包括：**获客、粘客和留客**。

1. 获客：如何进行拉新，通过更精准的营销获取客户。
2. 粘客：个性化推荐，搜索排序，场景运营等。
3. 留客：流失率预测，分析关键节点降低流失率。

如果按照**数据流处理的阶段来划分**用户画像建模的过程，可以分为**数据层、算法层和业务层**。你会发现在不同的层，都需要打上不同的标签。

**数据层**指的是用户消费行为里的标签。我们可以打上**“事实标签”**，作为数据**客观的记录**。
**算法层**指的是透过这些行为算出的用户建模。我们可以打上**“模型标签”**，作为用户画像的**分类标识**。
**业务层**指的是获客、粘客、留客的手段。我们可以打上**“预测标签”**，作为**业务关联**的结果。

所以这个**标签化的流程**，就是通过数据层的“事实标签”，在算法层进行计算，打上“模型标签”的分类结果，最后指导业务层，得出“预测标签”。

![标签化流程](E:\Jennifer\other\notes\数据分析\media\标签化流程.png)

#### 美团外卖的用户画像

**用户的唯一标识**：当然主要是以用户的注册手机号为标准。这样美团和大众点评的账号体系就可以相通。

在集团内部，各部门之间的协作，尤其是用户数据打通是非常困难的，所以这里建议，如果希望大数据对各个部门都能赋能，一定要在集团的战略高度上，尽早就在最开始的顶层架构上，将用户标识进行统一，这样在后续过程中才能实现用户数据的打通。

按照**“用户消费行为分析”的准则**来进行设计。
1. **用户标签**：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进行的注册。
2. **消费标签**：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等级。
3. **行为标签**：点外卖时间段、使用频次、平均点餐用时、访问路径。
4. **内容分析**：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。

有了这些数据，你就可以更好地理解业务，就可以预测用户的行为。

**业务层**上，我们都可以基于标签产生哪些业务价值呢？

- 在**获客**上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。

- 在**粘客**上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。

- 在**留客**上，预测用户是否可能会从平台上流失。在营销领域，关于用户留存有一个观点——如果将顾客流失率降低 5%，公司利润将提升 25%~85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。

“用户消费行为标签”都是基于一般情况考虑的，除此之外，用户的行为也会**随着营销的节奏产生异常值**，比如双十一的时候，如果商家都在促销就会产生突发的大量订单。因此在做用户画像的时候，还要考虑到异常值的处理。

总之，我们的最终目的不是处理这些数据，而是理解、使用这些数据挖掘的结果。对数据的标签化能让我们快速理解一个用户，一个商品，乃至一个视频内容的特征，从而方便我们去理解和使用数据。对数据的标签化其实考验的是我们的抽象能力。

### 数据采集

用户画像建模之前我们都要进行数据采集。数据采集是数据挖掘的**基础**，没有数据，挖掘也没有意义。很多时候，我们拥有多少数据源，多少数据量，以及数据质量如何，将决定我们挖掘产出的成果会怎样。

一个数据的走势，是由多个维度影响的。我们需要通过**多源**的数据采集，收集到尽可能多的数据维度，同时**保证数据的质量**，这样才能得到高质量的数据挖

从数据采集角度来说，我将数据源分成了以下的四类：**开放数据源、爬虫抓取、传感器和日志采集**。它们各有特点。

![四类数据源](E:\Jennifer\other\notes\数据分析\media\四类数据源.png)

**开放数据源**一般是**针对行业**的数据库。比如美国人口调查局开放了美国的人口信息、地区分布和教育情况数据。除了**政府**外，**企业**和**高校**也会开放相应的大数据，这方面北美相对来说做得好一些。国内，贵州做了不少大胆尝试，搭建了云平台，逐年开放了旅游、交通、商务等领域的数据量。

**爬虫抓取**，一般是**针对特定的网站或 App**。如果我们想要抓取指定的网站数据，比如购物网站上的购物评价等，就需要我们做特定的爬虫抓取。

**传感器**，它基本上采集的是**物理信息**。比如图像、视频、或者某个物体的速度、热度、压强等。

**日志采集**，这个是统计用户的操作。我们可以在**前端进行埋点**，在**后端进行脚本收集、统计**，来分析网站的访问情况，以及使用瓶颈等。

#### 使用开放数据源

开放数据源可以从两个维度来考虑，一个是**单位的维度**，比如政府、企业、高校；一个就是**行业维度**，比如交通、金融、能源等领域。这方面，国外的开放数据源比国内做得好一些，当然近些年国内的政府和高校做开放数据源的也越来越多。一方面服务社会，另一方面自己的影响力也会越来越大。

#### 用爬虫做抓取

在 Python 爬虫中，基本上会经历三个过程。
1. **使用 Requests 爬取内容**。我们可以使用 Requests 库来抓取网页信息。Requests 库可以说是 Python 爬虫的利器，也就是 Python 的 HTTP 库，通过这个库爬取网页中的数据，非常方便，可以帮我们节约大量的时间。
2. **使用 XPath 解析内容**。XPath 是 XML Path 的缩写，也就是 XML 路径语言。它是一种用来确定 XML 文档中某部分位置的语言，在开发中经常用来当作小型查询语言。XPath 可以通过元素和属性进行位置索引。
3. **使用 Pandas 保存数据**。Pandas 是让数据分析工作变得更加简单的高级数据结构，我们可以用 Pandas 保存爬取的数据。最后通过 Pandas 再写入到 XLS 或者 MySQL 等数据库中。

当然做 Python 爬虫还有很多利器，比如 Selenium，PhantomJS，或者用 Puppteteer 这种无头模式。

不编程就抓取到网页信息，三款常用的抓取工具：

- 火车采集器：<http://www.locoy.com/>

老牌的采集工具。它不仅可以做抓取工具，也可以做数据清洗、数据分析、数据挖掘和可视化等工作。数据源适用于绝大部分的网页，网页中能看到的内容都可以通过采集规则进行抓取。

- 八爪鱼<https://www.bazhuayu.com/>

免费的采集模板实际上就是内容采集规则，包括了电商类、生活服务类、社交媒体类和论坛类的网站都可以采集，用起来非常方便。当然你也可以自己来自定义任务。

云采集就是当你配置好采集任务，就可以交给八爪鱼的云端进行采集。八爪鱼一共有 5000 台服务器，通过云端多节点并发采集，采集速度远远超过本地采集。此外还可以自动切换多个 IP，避免 IP 被封，影响采集。

做过工程项目的同学应该能体会到，云采集这个功能太方便了，很多时候**自动切换 IP 以及云采集才是自动化采集的关键**。

- 集搜客<http://www.gooseeker.com/>

这个工具的特点是完全**可视化操作**，无需编程。整个采集过程也是所见即所得，抓取结果信息、错误信息等都反应在软件中。相比于八爪鱼来说，集搜客没有流程的概念，用户只需要关注抓取什么数据，而流程细节完全交给集搜客来处理。但是集搜客的缺点是没有云采集功能，所有爬虫都是在用户自己电脑上跑的。

#### 使用日志采集工具

日志采集最大的作用，就是**通过分析用户访问情况，提升系统的性能，从而提高系统承载量。及时发现系统承载瓶颈，也可以方便技术人员基于用户实际的访问情况进行优化**。

日志采集也是运维人员的重要工作之一，它记录了用户访问网站的全过程：哪些人在什么时间，通过什么渠
道（比如搜索引擎、网址输入）来过，都执行了哪些操作；系统是否产生了错误；甚至包括用户的 IP、HTTP 请求的时间，用户代理等。这些日志数据可以被写在一个日志文件中，也可以分成不同的日志文件，比如访问日志、错误日志等。

日志采集可以分**两种形式**。
1. 通过 **Web 服务器采集**，例如 httpd、Nginx、Tomcat 都自带日志记录功能。同时很多互联网企业都有自己的海量数据采集工具，多用于系统日志采集，如 Hadoop 的Chukwa、Cloudera 的 Flume、Facebook 的 Scribe 等，这些工具均采用分布式架构，能够满足每秒数百 MB 的日志数据采集和传输需求。
2. **自定义采集用户行为**，例如用 JavaScript 代码监听用户的行为、AJAX 异步请求后台记录日志等。

##### 埋点

埋点是日志采集的关键步骤，埋点**就是在有需要的位置采集相应的信息，进行上报**。比如某页面的访问情况，包括用户信息、设备信息；或者用户在页面上的操作行为，包括时间长短等。这就是埋点，每一个埋点就像一台摄像头，采集用户行为数据，将数据进行多维度的交叉分析，可真实还原出用户使用场景，和用户使用需求。

埋点就是在你**需要统计数据的地方植入统计代码**，当然植入代码可以自己写，也可以使用第三方统计工具。一般来说需要自己写的代码，一般是主营核心业务，对于埋点这类监测性的工具，市场上已经比较成熟，这里推荐你使用第三方的工具，比如**友盟、Google Analysis、Talkingdata** 等。他们都是采用前端埋点的方式，然后在第三方工具里就可以看到用户的行为数据。但如果我们想要看到更深层的用户操作行为，就需要进行自定义埋点。

日志采集有助于我们了解用户的操作数据，适用于运维监控、安全审计、业务数据分析等场景。一般 Web 服务器会自带日志功能，也可以使用 Flume 从不同的服务器集群中采集、汇总和传输大容量的日志数据。当然我们也可以使用第三方的统计工具或自定义埋点得到自己想要的统计内容。

#### 八爪鱼实战

##### 基本操作

基本上不需要编写代码，除了在正则表达式匹配的时候会用到 XPath。

XPath 的英文是 XML Path Language，也就是 XML 的路径语言，用来在 XML 文件中寻找我们想要的元素。所以八爪鱼可以使用 XPath 帮我们更灵活地定位我们想要找的元素。

###### 自定义任务 VS 简易采集

如果你想要采集数据就需要新建一个任务，在建任务的时候，八爪鱼会给你一个提示，是使用八爪鱼自带的“简易采集”，还是自定义一个任务。

简易采集集成了一些热门的模板，也就是我们经常访问的一些网站。只需要我们告诉工具两个信息即可，一个是需要采集的网址，另一个是登录网站的账号和密码。虽然简易采集比较方便快捷，但通常还是推荐使用自定义任务的方式，这样可以更灵活地帮我们提取想要的信息，比如你只想采集关于“D&G”的微博评论。

###### 流程

1. **输入网页**
2. **设计流程**：你需要告诉八爪鱼，你是如何操作页面的、想要提取页面上的哪些信息等。因为数据条数比较多，通常你还需要翻页，所以要进行循环翻页的设置。在设计流程中，你可以使用简易采集方式，也就是八爪鱼自带的模板，也可以采用自定义的方式。
3. **启动采集**：设计好采集流程后，就可以启动采集任务了，任务结束后，八爪鱼会提示你保存采集好的数据，通常是 xlsx 或 csv 格式。

八爪鱼的流程步骤有两类，可以划分为**基本步骤**和**高级步骤**。

- 基本步骤就是最常用的步骤，每次采集都会用到，一共分为 4 步，分别是**打开网页、点击元素、循环翻页、提取数据**。
- 高级步骤是辅助步骤，可以帮我们更好地对数据进行提取，比如网页输入框中输入关键词，验证码识别，下拉选项筛选想要的数据，或者某些判断条件下（比如存在某个关键词）才触发的采集等。这些操作可以**更精细化**地提取想要的数据。

![八爪鱼流程步骤](E:\Jennifer\other\notes\数据分析\media\八爪鱼流程步骤.png)

**打开网页**：新建任务，当你输入网址后，八爪鱼就会自动建立一个“打开网页”的流程。

**点击元素**：使用这个步骤是你在搜索或者提交某个请求。当你点击元素后，八爪鱼会提示你想要达到的目的：点击该按钮、采集该元素文本、还是鼠标移到该链接上。然后再选择“点击该按钮”进行确认即可。

**循环翻页**：通常你需要找到翻页的位置，点击它，会提示你“循环点击下一页”、“采集该链接文本”还是“点击该链接”。你需要确认这里是进行的“循环点击下一页”。

**提取数据**：在网页上选择你想要提取的页面范围，鼠标移动到页面上会呈现蓝色的阴影面积，它表明了你想提取的数据范围。然后点击鼠标后，在右侧选择“采集数据”即可。

建议：

1. 尽量使用用户操作视角进行模拟的方式进行操作，而不是在“流程视图”中手动创建相应的步骤。因为八爪鱼最大的特点就是所见即所得，所以一切就按照用户使用的流程进行操作即可。
2. 使用“流程视图”方便管理和调整。右侧有“流程视图”的按钮，点击之后进入到流程视图，会把你之前的操作以流程图的方式展示出来。你还可以调整每个步骤的参数，比如你之前的网址写错了想要修改，或者之前输入的文字需要调整等。

我们可以提前在八爪鱼工具里登录，这样再进行抓取的时候就是登录的状态，直接进行采集就可以了。

##### XPath

在八爪鱼工具中内置了 XPath 引擎，所以在我们用可视化方式选择元素的时候，会自动生成相应的 XPath 路径。当然我们也可以查看这些元素的 XPath，方便对它们进行精细地控制。

因为有时候我们采集的网站页面是不规律的，所以有时需要自己来定义 XPath 。

在八爪鱼工具中，很多控件都有 XPath，最常用的还是循环和提取数据中的 XPath。

###### 循环中的 XPath

在微博采集这个例子中，我们用到了两种循环方式，一种是“循环翻页”，一种是“循环列表”。

在“循环翻页”中，你可以在“流程视图”中点击“循环翻页”的控件，看到右侧的“高级选项”中的 XPath。在微博采集这个例子中，循环翻页的 XPath 是//A[@class=‘next’]。

在“循环列表”中，我在提取数据的时候，出现了页面提示“还有 20 个相同元素”，这时我选择“选中全部”。相当于出现了 20 个元素的循环列表。所以你在流程视图中，可以会看到提取数据外层嵌套了个循环列表。同样我们可以看到循环列表的 XPath 是//DIV[@class=‘card-feed’]。

###### 提取数据的 XPath

当我们点击流程中的“提取数据”，可以看到有很多字段名称，XPath 实际上定位到了这些要采集的字段。所以你需要选中一个字段，才能看到它的 XPath。

现在你知道了，八爪鱼的匹配是基于 XPath 的，那么你也可以自己来调整 XPath 参数。这样当匹配不到想要的数据的时候，可以检查下是不是 XPath 参数设置的问题，可以手动调整，从而抓取到正确的元素。

#### Python爬虫

爬虫实际上是用浏览器访问的方式模拟了访问网站的过程，整个过程包括三个阶段：打开网页、提取数据和保存数据。

**打开网页**：可以使用 **Requests** 访问页面，得到服务器返回给我们的数据，这里包括 HTML 页面以及 JSON 数据。

**提取数据**：主要用到了两个工具。针对 HTML 页面，可以使用 **XPath**进行元素定位，提取数据；针对 JSON 数据，可以使用 **JSON** 进行解析。

**保存数据**：我们可以使用 **Pandas** 保存数据，最后导出 CSV 文件。

##### Requests 访问页面

Requests 是 Python HTTP 的客户端库，编写爬虫的时候都会用到，编写起来也很简单。

```python
# Requests 是 Python HTTP 的客户端库
# 两种访问方式：Get 和 Post。
# Get 把参数包含在 url 中，而 Post 通过 request body 来传递参数。

# r.text 或 r.content 可以来获取HTML 的正文。
r = requests.get('http://www.douban.com')

# post
# data 的数据类型是个字典的结构，采用 key 和 value 的方式进行存储。
r = requests.post('http://xxx.com', data = {'key':'value'})
```

##### XPath

XPath 是 XML 的路径语言，实际上是通过元素和属性进行导航，帮我们定位位置。它有几种常用的路径表达方式。

![XPath](E:\Jennifer\other\notes\数据分析\media\XPath.png)

比如：

1. xpath(‘node’) 选取了 node 节点的所有子节点；
2. xpath(’/div’) 从根节点上选取 div 节点；
3. xpath(’//div’) 选取所有的 div 节点；
4. xpath(’./div’) 选取当前节点下的 div 节点；
5. xpath(’…’) 回到上一个节点；
6. xpath(’//@id’) 选取所有的 id 属性；
7. xpath(’//book[@id]’) 选取所有拥有名为 id 的属性的 book 元素；
8. xpath(’//book[@id=“abc”]’) 选取所有 book 元素，且这些 book 元素拥有 id=
"abc"的属性；
9. xpath(’//book/title | //book/price’) 选取 book 元素的所有 title 和 price 元素。

XPath 的选择功能非常强大，它可以提供超过 100个内建函数，来做匹配。我们想要定位的节点，几乎都可以使用 XPath 来选择。

使用 XPath 定位，你会用到 Python 的一个解析库 lxml。这个库的解析效率非常高，只需要调用 HTML 解析命令即可，然后再对 HTML 进行 XPath 函数的调用。

比如我们想要定位到 HTML 中的所有列表项目，可以采用下面这段代码。

```python
from lxml import etree
html = etree.HTML(html)
result = html.xpath('//li')
```

##### JSON对象

在 Python 中有 JSON 库，可以让我们将 Python 对象和 JSON 对象进行转换。，我们对数据进行解析就更方便了。

```python
json.dumps() # 转为json
json.loads() # 转为python对象

import json
jsonData = '{"a":1,"b":2,"c":3,"d":4,"e":5}';
input = json.loads(jsonData)
print input
```

##### 使用 JSON 数据自动下载王祖贤的海报

步骤：

1. 打开网页；
2. 输入关键词“王祖贤”；
3. 在搜索结果页中选择“图片”；
4. 下载图片页中的所有海报。

如果爬取的页面是动态页面，就需要关注 XHR 数据。因为动态页面的原理就是通过原生的 XHR 数据对象发出 HTTP 请求，得到服务器返回的数据后，再进行处理。XHR 会用于在后台与服务器交换数据。

https://www.douban.com/j/search_photo?q=%E7%8E%8B%E7%A5%96%E8%B4%A4&limit=20&start=0

数据被放到了 images 对象里，它是个数组的结构，每个数组的元素是个字典的类型，分别告诉了 src、author、url、id、title、width 和height 字段，这些字段代表的含义分别是原图片的地址、作者、发布地址、图片 ID、标题、图片宽度、图片高度等信息。

https://www.douban.com/j/search_photo?q= 王祖贤 &limit=20&start=0

网址中有三个参数：q、limit 和 start。start 实际上是请求的起始 ID，这里我们注意到它对图片的顺序标识是从 0 开始计算的。所以如果你想要从第 21 个图片进行下载，你可以将 start 设置为 20。可以写个 for 循环来跑完所有的请求，具体的代码如下：

```python
# coding:utf-8
import requests
import json
query = '王祖贤'
''' 下载图片 '''
def download(src, id):
    dir = './' + str(id) + '.jpg'
    try:
        pic = requests.get(src, timeout=10)
        fp = open(dir, 'wb')
        fp.write(pic.content)
        fp.close()
    except requests.exceptions.ConnectionError:
    	print('图片无法下载')
''' for 循环 请求全部的 url '''
for i in range(0, 22471, 20):
    url = 'https://www.douban.com/j/search_photo?q='+query+'&limit=20&start='+str(i)
    html = requests.get(url).text # 得到返回结果
    response = json.loads(html,encoding='utf-8') # 将 JSON 格式转换成 Python 对象
    for image in response['images']:
        print(image['src']) # 查看当前下载的图片网址
        download(image['src'], image['id']) # 下载一张图片
```

































